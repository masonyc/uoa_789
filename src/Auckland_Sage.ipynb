{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score\n",
    "from typing import Optional, Tuple, Union\n",
    "import calendar\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler as RawNeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# output_path = \"/run/media/yunchen/lacie\"\n",
    "output_path = \".\"\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "HIDDEN_LAYER = 16\n",
    "NUM_LAYERS = 5\n",
    "NEIGHBOUR_SIZE = [5,5,5,5,5]\n",
    "DROP_OUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,      2,      0,  ..., 463632, 463634, 463635],\n",
      "        [     2,      0,      1,  ..., 463631, 463635, 463634]])\n",
      "tensor([ 96.8730,  96.8730, 100.7870,  ...,  20.1610,  16.2460,  16.2460])\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = pd.read_csv(f\"{output_path}/datasets/akl_prewalked_nodes.csv\")\n",
    "street_edges_df = pd.read_csv(f\"{output_path}/datasets/akl_street_edges.csv\")\n",
    "source_street_index, targe_street_index, street_distance_weight = street_edges_df[\"source_street\"], street_edges_df[\n",
    "    \"target_street\"], street_edges_df[\"distance\"]\n",
    "\n",
    "# isolated_list = []\n",
    "# non_isolated_source = set(source_street_index.values.tolist())\n",
    "# non_isolated_target = set(targe_street_index.values.tolist())\n",
    "# for isolated_i,_ in street_nodes_df.iterrows():\n",
    "#     if isolated_i not in non_isolated_source and isolated_i  not in non_isolated_target:\n",
    "#         isolated_list.append(isolated_i)\n",
    "# street_nodes_df.drop(axis=0,index = isolated_list,inplace=True)\n",
    "\n",
    "street_edges_source_index_tensor = torch.tensor([source_street_index.values.tolist()])\n",
    "street_edges_target_index_tensor = torch.tensor([targe_street_index.values.tolist()])\n",
    "street_edges_index_tensor = torch.cat((street_edges_source_index_tensor, street_edges_target_index_tensor), 0)\n",
    "street_edges_weight_tensor = torch.tensor(street_distance_weight.values.tolist())\n",
    "\n",
    "print(street_edges_index_tensor)\n",
    "print(street_edges_weight_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Positive POI 16547  Negative POI 447092\n",
      "Layer 1 Positive POI 31350  Negative POI 432289\n",
      "Layer 2 Positive POI 52923  Negative POI 410716\n",
      "Layer 3 Positive POI 71277  Negative POI 392362\n",
      "Layer 4 Positive POI 87508  Negative POI 376131\n",
      "Layer 5 Positive POI 101944  Negative POI 361695\n"
     ]
    }
   ],
   "source": [
    "count_poi_df = street_nodes_df.copy()\n",
    "count_poi_df[\"poi_count\"] = count_poi_df.apply(lambda row:row.amenity+row.restaurant+row.education+row.healthcare+row.shop+row.cloth,axis=1)\n",
    "positive_nodes_with_poi_df_layer0 = count_poi_df[count_poi_df[\"poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer0 = count_poi_df[count_poi_df[\"poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer1 = count_poi_df[count_poi_df[\"Layer_1_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer1 = count_poi_df[count_poi_df[\"Layer_1_agg_poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer2 = count_poi_df[count_poi_df[\"Layer_2_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer2 = count_poi_df[count_poi_df[\"Layer_2_agg_poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer3 = count_poi_df[count_poi_df[\"Layer_3_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer3 = count_poi_df[count_poi_df[\"Layer_3_agg_poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer4 = count_poi_df[count_poi_df[\"Layer_4_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer4 = count_poi_df[count_poi_df[\"Layer_4_agg_poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer5 = count_poi_df[count_poi_df[\"Layer_5_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer5 = count_poi_df[count_poi_df[\"Layer_5_agg_poi_count\"]<=0]\n",
    "print(f\"Layer 0 Positive POI {len(positive_nodes_with_poi_df_layer0)}  Negative POI {len(negative_nodes_without_poi_df_layer0)}\")\n",
    "print(f\"Layer 1 Positive POI {len(positive_nodes_with_poi_df_layer1)}  Negative POI {len(negative_nodes_without_poi_df_layer1)}\")\n",
    "print(f\"Layer 2 Positive POI {len(positive_nodes_with_poi_df_layer2)}  Negative POI {len(negative_nodes_without_poi_df_layer2)}\")\n",
    "print(f\"Layer 3 Positive POI {len(positive_nodes_with_poi_df_layer3)}  Negative POI {len(negative_nodes_without_poi_df_layer3)}\")\n",
    "print(f\"Layer 4 Positive POI {len(positive_nodes_with_poi_df_layer4)}  Negative POI {len(negative_nodes_without_poi_df_layer4)}\")\n",
    "print(f\"Layer 5 Positive POI {len(positive_nodes_with_poi_df_layer5)}  Negative POI {len(negative_nodes_without_poi_df_layer5)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Layer_5_agg_restaurant', 'Layer_5_agg_amenity',\n",
      "       'Layer_5_agg_education', 'Layer_5_agg_healthcare', 'Layer_5_agg_shop',\n",
      "       'Layer_5_agg_cloth', 'Layer_5_agg_average_poi_distance'],\n",
      "      dtype='object')\n",
      "tensor([[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "         2646.8850],\n",
      "        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "         3347.5391],\n",
      "        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "         1734.5990],\n",
      "        ...,\n",
      "        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "          163.6040],\n",
      "        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "          259.6060],\n",
      "        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "          118.7930]])\n",
      "463639\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = street_nodes_df[[\"Layer_5_agg_restaurant\", \"Layer_5_agg_amenity\",\"Layer_5_agg_education\",\"Layer_5_agg_healthcare\",\"Layer_5_agg_shop\",\"Layer_5_agg_cloth\",\"Layer_5_agg_average_poi_distance\"]]\n",
    "\n",
    "street_nodes_df_copy = street_nodes_df.copy()\n",
    "print(street_nodes_df_copy.columns)\n",
    "\n",
    "street_nodes_features_tensor = torch.tensor(street_nodes_df_copy.values.tolist())\n",
    "\n",
    "neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "                                                torch.tensor(int(0), dtype=torch.int32))\n",
    "\n",
    "number_of_nodes = len(street_nodes_features_tensor)\n",
    "number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "print(street_nodes_features_tensor)\n",
    "print(number_of_nodes)\n",
    "print(number_of_node_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# street_nodes_df = street_nodes_df[street_nodes_df.columns[4:]]\n",
    "#\n",
    "# street_nodes_df_copy = street_nodes_df.copy()\n",
    "# print(street_nodes_df_copy.head)\n",
    "# # street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\",\"x\",\"y\"], axis=1, inplace=True)\n",
    "# street_nodes_df_copy.drop([\"street_length\",\"Average_POI_Distance\"], axis=1, inplace=True)\n",
    "# print(street_nodes_df_copy.columns)\n",
    "#\n",
    "# street_nodes_features_tensor = torch.tensor(street_nodes_df_copy.values.tolist())\n",
    "#\n",
    "# number_of_nodes = len(street_nodes_features_tensor)\n",
    "# number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "# print(street_nodes_features_tensor)\n",
    "# print(number_of_nodes)\n",
    "# print(number_of_node_features)\n",
    "#\n",
    "# street_nodes_df_copy_2 = street_nodes_df.copy()\n",
    "# # street_nodes_df_copy_2[\"poi_count\"] = street_nodes_df_copy_2.apply(lambda row:row.amenity+row.restaurant+row.school+row.shop+row.healthcare+row.clothes,axis=1)\n",
    "# street_nodes_df_copy_2[\"poi_count\"] = street_nodes_df_copy_2.apply(lambda row:row.amenity+row.restaurant+row.education+row.healthcare+row.shop+row.cloth,axis=1)\n",
    "# positive_nodes_with_poi_df_layer = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]>0]\n",
    "# negative_nodes_without_poi_df_layer = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]<=0]\n",
    "# positive_nodes_index = torch.tensor(positive_nodes_with_poi_df.index)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# def custom_pos_sampling_with_POI(\n",
    "#         edge_weight: Tensor,\n",
    "#         batch: Tensor,\n",
    "# ):\n",
    "#     pos_node_seq = []\n",
    "#     neg_node_seq = []\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_seq = [start_node_id.item()]\n",
    "#         current_node_id = current_node_seq[-1]\n",
    "#\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "#         len_neighbours_edge_index = len(neighbours_edge_index)\n",
    "#         neighbour_id_list = []\n",
    "#         for i in neighbours_edge_index:\n",
    "#             neighbour_id_list.append(select_neighbour_node_id_from_edge(i,current_node_id))\n",
    "#\n",
    "#         normalized_neighbour_poi_weights = calculate_normalized_poi_weights(neighbour_id_list)\n",
    "#\n",
    "#         global_node_without_poi = negative_nodes_without_poi_df.sample(replace=True).index.values[0]\n",
    "#         global_node_with_poi = positive_nodes_with_poi_df.sample(n=1,replace=True).index.values[0]\n",
    "#         # all neighbours don't have POI\n",
    "#         if np.isnan(normalized_neighbour_poi_weights).all():\n",
    "#             # Also no neighbour\n",
    "#             if len_neighbours_edge_index == 0:\n",
    "#                 pos_node_seq.append([current_node_id,global_node_without_poi])\n",
    "#                 neg_node_seq.append(global_node_with_poi)\n",
    "#                 # neg_node_seq.append(current_node_id)\n",
    "#                 # pos_node_seq.append([global_node_with_poi,global_node_with_poi])\n",
    "#                 continue\n",
    "#             # No neighbour has POIs but do have neighbours\n",
    "#             else:\n",
    "#                 # Pick a neighbour randomly using distance distribution to give future poi opportunity\n",
    "#                 neighbour_distance_weights = torch.index_select(edge_weight, 0, neighbours_edge_index).numpy()\n",
    "#                 norm_neighbour_distance_weights = calculate_normalized_distance_weights(neighbour_distance_weights)\n",
    "#                 neighbour_weights_index = np.random.choice(len(norm_neighbour_distance_weights),p=norm_neighbour_distance_weights)\n",
    "#\n",
    "#                 pos_node_seq.append([current_node_id,global_node_without_poi])\n",
    "#                 neg_node_seq.append(global_node_with_poi)\n",
    "#                 continue\n",
    "#         # normal case pick with poi distribution\n",
    "#         else:\n",
    "#             neighbour_weights_index = np.random.choice(len(normalized_neighbour_poi_weights),\n",
    "#                                                        p=normalized_neighbour_poi_weights)\n",
    "#\n",
    "#         next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "#         next_node_id = select_neighbour_node_id_from_edge(next_edge_index,current_node_id)\n",
    "#\n",
    "#         pos_node_seq.append([current_node_id,next_node_id])\n",
    "#         neg_node_seq.append(global_node_without_poi)\n",
    "#\n",
    "#     return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "#         np.asarray(neg_node_seq, dtype=np.int32))\n",
    "#\n",
    "# def calculate_normalized_distance_weights(neighbour_distance_weights):\n",
    "#     neighbour_distance_weights_sum = sum(neighbour_distance_weights)\n",
    "#     reverted_norm_neighbour_distance_weights = [1-(i / neighbour_distance_weights_sum) for i in neighbour_distance_weights]\n",
    "#\n",
    "#     reverted_norm_neighbour_distance_weights_sum = sum(reverted_norm_neighbour_distance_weights)\n",
    "#     norm_neighbour_distance_weights = [(i / reverted_norm_neighbour_distance_weights_sum) for i in reverted_norm_neighbour_distance_weights]\n",
    "#     return norm_neighbour_distance_weights\n",
    "#\n",
    "# def calculate_normalized_poi_weights(neighbour_id_list):\n",
    "#     neighbour_id_weights = []\n",
    "#     for neighbour_id in neighbour_id_list:\n",
    "#         poi_weight = 0\n",
    "#         neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "#                                                 torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "#         poi_weight += torch.sum(neighbour_features)\n",
    "#         neighbour_id_weights.append(poi_weight)\n",
    "#\n",
    "#     neighbour_poi_weights = np.array(neighbour_id_weights)\n",
    "#     neighbour_poi_weights_sum=sum(neighbour_poi_weights)\n",
    "#     normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
    "#     return normalized_neighbour_poi_weights\n",
    "#\n",
    "# def select_neighbour_node_id_from_edge(next_edge_index,current_node_id):\n",
    "#     next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "#     next_edge = next_edge_df.values[0]\n",
    "#     if next_edge[0] != current_node_id:\n",
    "#         neighbour_node_id = next_edge[0]\n",
    "#     else:\n",
    "#         neighbour_node_id = next_edge[1]\n",
    "#\n",
    "#     return neighbour_node_id\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def custom_sampling_with_Prewalk(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    "):\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        current_node_id = current_node_seq[-1]\n",
    "\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "        neighbour_id_list = []\n",
    "        for i in neighbours_edge_index:\n",
    "            neighbour_id_list.append(select_neighbour_node_id_from_edge(i,current_node_id))\n",
    "\n",
    "        normalized_neighbour_poi_weights = calculate_normalized_poi_weights(neighbour_id_list)\n",
    "\n",
    "        global_node_without_poi_layer5 = negative_nodes_without_poi_df_layer5.sample(replace=True).index.values[0]\n",
    "        global_node_with_poi_layer5 = positive_nodes_with_poi_df_layer5.sample(replace=True).index.values[0]\n",
    "        # all neighbours don't have POI\n",
    "        if np.isnan(normalized_neighbour_poi_weights).all():\n",
    "            pos_node_seq.append([current_node_id,global_node_without_poi_layer5])\n",
    "            neg_node_seq.append(global_node_with_poi_layer5)\n",
    "            continue\n",
    "        # normal case pick with poi distribution\n",
    "        else:\n",
    "            neighbour_weights_index = np.random.choice(len(normalized_neighbour_poi_weights),\n",
    "                                                       p=normalized_neighbour_poi_weights)\n",
    "\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_node_id = select_neighbour_node_id_from_edge(next_edge_index,current_node_id)\n",
    "\n",
    "        pos_node_seq.append([current_node_id,next_node_id])\n",
    "        neg_node_seq.append(global_node_without_poi_layer5)\n",
    "\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "        np.asarray(neg_node_seq, dtype=np.int32))\n",
    "\n",
    "def calculate_normalized_poi_weights(neighbour_id_list):\n",
    "    neighbour_id_weights = []\n",
    "    for neighbour_id in neighbour_id_list:\n",
    "        poi_weight = 0\n",
    "        neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "                                                torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "        neighbour_features = neighbour_features[0][:-1]\n",
    "        poi_weight += torch.sum(neighbour_features)\n",
    "        neighbour_id_weights.append(poi_weight)\n",
    "\n",
    "    neighbour_poi_weights = np.array(neighbour_id_weights)\n",
    "    neighbour_poi_weights_sum=sum(neighbour_poi_weights)\n",
    "    normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
    "    return normalized_neighbour_poi_weights\n",
    "\n",
    "def select_neighbour_node_id_from_edge(next_edge_index,current_node_id):\n",
    "    next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "    next_edge = next_edge_df.values[0]\n",
    "    if next_edge[0] != current_node_id:\n",
    "        neighbour_node_id = next_edge[0]\n",
    "    else:\n",
    "        neighbour_node_id = next_edge[1]\n",
    "\n",
    "    return neighbour_node_id\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RawNeighborSampler This module iteratively samples neighbors (at each layer) and constructs bipartite graphs that simulate the actual computation flow of GNNs.\n",
    "\n",
    "format-selected)\n",
    "NeighborSampler holds the current :obj:batch_size, the IDs :obj:n_id of all nodes involved in the computation, and a list of bipartite graph objects via the tuple :obj:(edge_index, e_id, size), where :obj:edge_index represents the bipartite edges between source and target nodes, :obj:e_id denotes the IDs of original edges in the full graph, and :obj:size holds the shape of the bipartite graph.\n",
    "\n",
    "The actual computation graphs are then returned in reverse-mode, meaning that we pass messages from a larger set of nodes to a smaller one, until we reach the nodes for which we originally wanted to compute embeddings.\n",
    "https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "https://towardsdatascience.com/pytorch-geometric-graph-embedding-da71d614c3a\n",
    "https://gist.github.com/anuradhawick/904e7f2d2101f4b76516d04046007426\n",
    "https://zhuanlan.zhihu.com/p/387262710\n",
    "\"\"\"\n",
    "\n",
    "class NeighborSampler(RawNeighborSampler):\n",
    "    def sample(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "\n",
    "        pos_batch, neg_batch = custom_sampling_with_Prewalk(street_edges_weight_tensor, batch)\n",
    "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
    "        return super(NeighborSampler,self).sample(batch)\n",
    "\n",
    "train_loader = NeighborSampler(street_edges_index_tensor, sizes=NEIGHBOUR_SIZE, batch_size=256, num_nodes=number_of_nodes,\n",
    "                                num_workers=6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            # print(f\"x_target {x_target}\")\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SAGE(number_of_node_features, hidden_channels=HIDDEN_LAYER, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x, edge_index = street_nodes_features_tensor.to(device), street_edges_index_tensor.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def regression_train(embedding_df):\n",
    "    AKL_df = pd.read_csv(f\"{output_path}/datasets/property_data_with_street.csv\", encoding='latin1')\n",
    "    AKL_df = AKL_df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    akl_embedding_np = embedding_df.numpy()  #convert to Numpy array\n",
    "    akl_embedding_df = pd.DataFrame(akl_embedding_np)  #convert to a dataframe\n",
    "    embedding_size = akl_embedding_df.shape[1]\n",
    "    akl_embedding_df.columns = ['street_embedding_' + str(i) for i in range(embedding_size)]\n",
    "\n",
    "    akl_street_nodes_df = pd.read_csv(f\"{output_path}/datasets/akl_street_nodes.csv\")\n",
    "    akl_street_nodes_df = akl_street_nodes_df.rename(columns={\"source\": \"street_sources\", \"target\": \"street_targets\"})\n",
    "    AKL_df = find_embedding_for_property(AKL_df, akl_street_nodes_df, akl_embedding_df)\n",
    "    property_columns = ['CL_Suburb', 'CL_Sale_Tenure', 'CL_Sale_Date', 'CL_Land_Valuation_Capital_Value',\n",
    "                        'CL_Building_Floor_Area', 'CL_Building_Site_Cover',\n",
    "                        'CL_Land_Area', 'CL_Bldg_Const', 'CL_Bldg_Cond', 'CL_Roof_Const', 'CL_Roof_Cond',\n",
    "                        'CL_Category', 'CL_LUD_Age', 'CL_LUD_Land_Use_Description',\n",
    "                        'CL_MAS_No_Main_Roof_Garages', 'CL_Bedrooms', 'CL_Bathrooms'] + ['street_embedding_' + str(i)\n",
    "                                                                                         for i in range(embedding_size)]\n",
    "    X_columns = AKL_df[property_columns].values\n",
    "    #print(X_columns)\n",
    "    Y_column = AKL_df['Log_Sale_Price_Net'].values\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_columns, Y_column, test_size=0.2, random_state=1,\n",
    "                                                        shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1, shuffle=True)\n",
    "    hedonic_regression = LinearRegression()\n",
    "    hedonic_regression.fit(X_train, Y_train)\n",
    "\n",
    "    hedonic_regression_validation_result = hedonic_regression.predict(X_val)\n",
    "\n",
    "    validation_RMSE = round(mean_squared_error(Y_val, hedonic_regression_validation_result), 4)\n",
    "    validation_R2 = round(r2_score(Y_val, hedonic_regression_validation_result), 4)\n",
    "    return validation_RMSE, validation_R2\n",
    "\n",
    "\n",
    "def find_embedding_for_property(property_df, street_df, emb_df):\n",
    "    street_with_embedding = street_df.merge(emb_df, left_index=True, right_index=True)\n",
    "    output_df = property_df.merge(street_with_embedding, on=[\"street_sources\", \"street_targets\"])\n",
    "    return output_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22699/3799742874.py:51: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_22699/3799742874.py:51: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_22699/3799742874.py:51: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_22699/3799742874.py:51: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_22699/3799742874.py:51: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_22699/3799742874.py:51: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    # i=0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # i+=1\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x[n_id].to(device), adjs)\n",
    "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "        # print(i)\n",
    "    return total_loss / number_of_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_model_embedding():\n",
    "    model.eval()\n",
    "    embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "historical_rmse, historical_r2, historical_loss = np.inf, np.inf, np.inf\n",
    "for epoch in range(1, 20):\n",
    "    loss = train()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        temp_embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "        regression_rmse, regression_r2 = regression_train(temp_embedding)\n",
    "\n",
    "        if historical_rmse == np.inf:\n",
    "            historical_rmse = regression_rmse\n",
    "\n",
    "        if historical_r2 == np.inf:\n",
    "            historical_r2 = regression_r2\n",
    "\n",
    "        if historical_loss == np.inf:\n",
    "            historical_loss = loss\n",
    "\n",
    "        if loss > historical_loss:\n",
    "            # if regression_rmse > historical_rmse:\n",
    "            #     print(\n",
    "            #         f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on RMSE! Current RMSE is {regression_rmse}, previous RMSE is {historical_rmse} ')\n",
    "            #     break\n",
    "\n",
    "            if regression_r2 < historical_r2:\n",
    "                print(\n",
    "                    f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on r2! Current R2 is {regression_r2}, previous R2 is {historical_r2} ')\n",
    "                break\n",
    "        else:\n",
    "            historical_loss = loss\n",
    "            historical_rmse = regression_rmse\n",
    "            historical_r2 = regression_r2\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},Current RMSE is {regression_rmse}, Current R2 is {regression_r2} ')\n",
    "        output_embedding = temp_embedding\n",
    "\n",
    "# output_embedding = get_model_embedding()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(output_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_np = output_embedding.numpy()  #convert to Numpy array\n",
    "output_df = pd.DataFrame(output_np)  #convert to a dataframe\n",
    "current_GMT = time.gmtime()\n",
    "ts = calendar.timegm(current_GMT)\n",
    "output_df.to_csv(f\"./outputs/embeddings/prewalk/with_amenity_filters/akl_embedding_{ts}.csv\", index=False)  #save to file\n",
    "print(f\"akl_embedding_{ts}.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
