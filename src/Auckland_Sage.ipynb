{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score\n",
    "from typing import Optional, Tuple, Union\n",
    "import calendar\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler as RawNeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# output_path = \"/run/media/yunchen/lacie\"\n",
    "output_path = \".\"\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "HIDDEN_LAYER =16\n",
    "NUM_LAYERS = 5\n",
    "NEIGHBOUR_SIZE = [5, 5]\n",
    "DROP_OUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,      2,      0,  ..., 458250, 458250, 458251],\n",
      "        [     2,      0,      1,  ..., 458249, 458251, 458250]])\n",
      "tensor([ 96.8730,  96.8730, 100.7870,  ..., 328.7770, 493.5280, 493.5280])\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = pd.read_csv(f\"{output_path}/datasets/akl_street_nodes.csv\")\n",
    "street_edges_df = pd.read_csv(f\"{output_path}/datasets/akl_street_edges.csv\")\n",
    "source_street_index, targe_street_index, street_distance_weight = street_edges_df[\"source_street\"], street_edges_df[\n",
    "    \"target_street\"], street_edges_df[\"distance\"]\n",
    "\n",
    "# isolated_list = []\n",
    "# non_isolated_source = set(source_street_index.values.tolist())\n",
    "# non_isolated_target = set(targe_street_index.values.tolist())\n",
    "# for isolated_i,_ in street_nodes_df.iterrows():\n",
    "#     if isolated_i not in non_isolated_source and isolated_i  not in non_isolated_target:\n",
    "#         isolated_list.append(isolated_i)\n",
    "# street_nodes_df.drop(axis=0,index = isolated_list,inplace=True)\n",
    "\n",
    "street_edges_source_index_tensor = torch.tensor([source_street_index.values.tolist()])\n",
    "street_edges_target_index_tensor = torch.tensor([targe_street_index.values.tolist()])\n",
    "street_edges_index_tensor = torch.cat((street_edges_source_index_tensor, street_edges_target_index_tensor), 0)\n",
    "street_edges_weight_tensor = torch.tensor(street_distance_weight.values.tolist())\n",
    "\n",
    "print(street_edges_index_tensor)\n",
    "print(street_edges_weight_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         street_length  restaurant    x    y  Average_POI_Distance  amenity  \\\n",
      "0              32.641         0.0  0.0  0.0                32.641      0.0   \n",
      "1              68.146         0.0  0.0  0.0                68.146      0.0   \n",
      "2              64.232         0.0  0.0  0.0                64.232      0.0   \n",
      "3              50.560         0.0  0.0  0.0                50.560      0.0   \n",
      "4              20.232         0.0  0.0  0.0                20.232      0.0   \n",
      "...               ...         ...  ...  ...                   ...      ...   \n",
      "458247         25.652         0.0  0.0  0.0                25.652      0.0   \n",
      "458248        166.770         0.0  0.0  0.0               166.770      0.0   \n",
      "458249        143.628         0.0  0.0  0.0               143.628      0.0   \n",
      "458250        185.149         0.0  0.0  0.0               185.149      0.0   \n",
      "458251        308.379         0.0  0.0  0.0               308.379      0.0   \n",
      "\n",
      "        school  shop  healthcare  clothes  \n",
      "0          0.0   0.0         0.0      0.0  \n",
      "1          0.0   0.0         0.0      0.0  \n",
      "2          0.0   0.0         0.0      0.0  \n",
      "3          0.0   0.0         0.0      0.0  \n",
      "4          0.0   0.0         0.0      0.0  \n",
      "...        ...   ...         ...      ...  \n",
      "458247     0.0   0.0         0.0      0.0  \n",
      "458248     0.0   0.0         0.0      0.0  \n",
      "458249     0.0   0.0         0.0      0.0  \n",
      "458250     0.0   0.0         0.0      0.0  \n",
      "458251     0.0   0.0         0.0      0.0  \n",
      "\n",
      "[458252 rows x 10 columns]>\n",
      "Index(['restaurant', 'amenity', 'school', 'shop', 'healthcare', 'clothes'], dtype='object')\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "458252\n",
      "6\n",
      "15269\n",
      "442983\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = street_nodes_df[street_nodes_df.columns[4:]]\n",
    "\n",
    "street_nodes_df_copy = street_nodes_df.copy()\n",
    "print(street_nodes_df_copy.head)\n",
    "street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\",\"x\",\"y\"], axis=1, inplace=True)\n",
    "# street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\"], axis=1, inplace=True)\n",
    "print(street_nodes_df_copy.columns)\n",
    "\n",
    "street_nodes_features_tensor = torch.tensor(street_nodes_df_copy.values.tolist())\n",
    "\n",
    "number_of_nodes = len(street_nodes_features_tensor)\n",
    "number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "print(street_nodes_features_tensor)\n",
    "print(number_of_nodes)\n",
    "print(number_of_node_features)\n",
    "\n",
    "street_nodes_df_copy_2 = street_nodes_df.copy()\n",
    "street_nodes_df_copy_2[\"poi_count\"] = street_nodes_df_copy_2.apply(lambda row:row.amenity+row.school+row.shop+row.healthcare+row.clothes,axis=1)\n",
    "positive_nodes_with_poi_df = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]>0]\n",
    "negative_nodes_without_poi_df = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]<=0]\n",
    "print(len(positive_nodes_with_poi_df))\n",
    "print(len(negative_nodes_without_poi_df))\n",
    "# positive_nodes_index = torch.tensor(positive_nodes_with_poi_df.index)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def custom_pos_sampling_with_POI(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    "):\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        current_node_id = current_node_seq[-1]\n",
    "\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "        len_neighbours_edge_index = len(neighbours_edge_index)\n",
    "        neighbour_id_list = []\n",
    "        for i in neighbours_edge_index:\n",
    "            neighbour_id_list.append(select_neighbour_node_id_from_edge(i,current_node_id))\n",
    "\n",
    "        normalized_neighbour_poi_weights = calculate_normalized_poi_weights(neighbour_id_list)\n",
    "\n",
    "        # all neighbours don't have POI\n",
    "        if np.isnan(normalized_neighbour_poi_weights).all():\n",
    "            global_node_with_poi = positive_nodes_with_poi_df.sample(n=1,replace=True).index.values[0]\n",
    "            # Also no neighbour\n",
    "            if len_neighbours_edge_index == 0:\n",
    "                pos_node_seq.append([current_node_id,current_node_id])\n",
    "                neg_node_seq.append(global_node_with_poi)\n",
    "                # neg_node_seq.append(current_node_id)\n",
    "                # pos_node_seq.append([global_node_with_poi,global_node_with_poi])\n",
    "                continue\n",
    "            # No neighbour has POIs but do have neighbours\n",
    "            else:\n",
    "                # Pick a neighbour randomly using distance distribution to give future poi opportunity\n",
    "                neighbour_distance_weights = torch.index_select(edge_weight, 0, neighbours_edge_index).numpy()\n",
    "                norm_neighbour_distance_weights = calculate_normalized_distance_weights(neighbour_distance_weights)\n",
    "                neighbour_weights_index = np.random.choice(len(norm_neighbour_distance_weights),p=norm_neighbour_distance_weights)\n",
    "\n",
    "                pos_node_seq.append([current_node_id,neighbour_weights_index])\n",
    "                neg_node_seq.append(global_node_with_poi)\n",
    "                continue\n",
    "        # normal case pick with poi distribution\n",
    "        else:\n",
    "            neighbour_weights_index = np.random.choice(len(normalized_neighbour_poi_weights),\n",
    "                                                       p=normalized_neighbour_poi_weights)\n",
    "\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_node_id = select_neighbour_node_id_from_edge(next_edge_index,current_node_id)\n",
    "\n",
    "        global_node_without_poi = negative_nodes_without_poi_df.sample(replace=True).index.values[0]\n",
    "\n",
    "        current_node_seq.append(next_node_id)\n",
    "        pos_node_seq.append(current_node_seq)\n",
    "        neg_node_seq.append(global_node_without_poi)\n",
    "\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "        np.asarray(neg_node_seq, dtype=np.int32))\n",
    "\n",
    "def calculate_normalized_distance_weights(neighbour_distance_weights):\n",
    "    neighbour_distance_weights_sum = sum(neighbour_distance_weights)\n",
    "    reverted_norm_neighbour_distance_weights = [1-(i / neighbour_distance_weights_sum) for i in neighbour_distance_weights]\n",
    "\n",
    "    reverted_norm_neighbour_distance_weights_sum = sum(reverted_norm_neighbour_distance_weights)\n",
    "    norm_neighbour_distance_weights = [(i / reverted_norm_neighbour_distance_weights_sum) for i in reverted_norm_neighbour_distance_weights]\n",
    "    return norm_neighbour_distance_weights\n",
    "\n",
    "def calculate_normalized_poi_weights(neighbour_id_list):\n",
    "    neighbour_id_weights = []\n",
    "    for neighbour_id in neighbour_id_list:\n",
    "        poi_weight = 0\n",
    "        neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "                                                torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "        poi_weight += torch.sum(neighbour_features)\n",
    "        neighbour_id_weights.append(poi_weight)\n",
    "\n",
    "    neighbour_poi_weights = np.array(neighbour_id_weights)\n",
    "    neighbour_poi_weights_sum=sum(neighbour_poi_weights)\n",
    "    normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
    "    return normalized_neighbour_poi_weights\n",
    "\n",
    "def select_neighbour_node_id_from_edge(next_edge_index,current_node_id):\n",
    "    next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "    next_edge = next_edge_df.values[0]\n",
    "    if next_edge[0] != current_node_id:\n",
    "        neighbour_node_id = next_edge[0]\n",
    "    else:\n",
    "        neighbour_node_id = next_edge[1]\n",
    "\n",
    "    return neighbour_node_id\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# def custom_pos_sampling(\n",
    "#         edge_weight: tensor,\n",
    "#         batch: tensor,\n",
    "# ) -> union[tensor, tuple[tensor, tensor]]:\n",
    "#     pos_node_seq = []\n",
    "#     neg_node_seq = []\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_seq = [start_node_id.item()]\n",
    "#         total_distance = 0\n",
    "#         current_node_id = start_node_id\n",
    "#         # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=true)[1]\n",
    "#\n",
    "#         # 选出edge对应的weight\n",
    "#         neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "#         norm_neighbour_weights = [i / sum(neighbour_weights.numpy()) for i in neighbour_weights.numpy()]\n",
    "#         #根据概率随机选一个\n",
    "#         #print(neighbours_edge_index,len(neighbour_weights))\n",
    "#         if len(neighbour_weights) == 0:\n",
    "#             current_node_seq.append(current_node_id)\n",
    "#             pos_node_seq.append(current_node_seq)\n",
    "#             #neg_node_seq.append(current_node_seq)\n",
    "#             continue\n",
    "#         neighbour_weights_index = np.random.choice(len(neighbour_weights), p=norm_neighbour_weights)\n",
    "#\n",
    "#         # print(\"current node id \\n\", current_node_id)\n",
    "#         #print(\"neighbour weights \\n\", neighbour_weights)\n",
    "#         #print(\"neighbour weights index  \\n\", neighbour_weights_index)\n",
    "#         #print(neighbour_weights.min(),neighbour_weights.argmin())\n",
    "#\n",
    "#         # 取最近的边\n",
    "#         # todo：加入别的策略，poi信息等\n",
    "#         next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "#         next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "#         #print(\"next edge \\n\", next_edge_df)\n",
    "#         next_edge = next_edge_df.values[0]\n",
    "#         total_distance += next_edge[2]\n",
    "#         # next_edge[0] = source street\n",
    "#         # next_edge[1] = target_street\n",
    "#         # next_edge[2] = distance\n",
    "#         if next_edge[0] != current_node_id:\n",
    "#             current_node_id = next_edge[0]\n",
    "#         else:\n",
    "#             current_node_id = next_edge[1]\n",
    "#         current_node_seq.append(current_node_id)\n",
    "#         pos_node_seq.append(current_node_seq)\n",
    "#     #if len(neg_node_seq) >0 :\n",
    "#     #print(\"isolated node: {number} {node_list}\".format(number = len(neg_node_seq),node_list = neg_node_seq))\n",
    "#     return torch.from_numpynumpy(np.asarray(pos_node_seq, dtype=np.int32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# def custom_pos_sampling_with_POI(\n",
    "#         edge_weight: Tensor,\n",
    "#         batch: Tensor,\n",
    "# ):\n",
    "#     pos_node_seq = []\n",
    "#     neg_node_seq = []\n",
    "#     poi_nodes = set()\n",
    "#     no_poi_nodes = set()\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_seq = [start_node_id.item()]\n",
    "#         current_node_id = current_node_seq[-1]\n",
    "#         # 找距离？\n",
    "#         # current_x,current_y = street_nodes_df.iloc[[start_node_id]][\"x\"],street_nodes_df.iloc[[start_node_id]][\"y\"]\n",
    "#         # print(start_node_id)\n",
    "#         # print(street_nodes_df.iloc[[start_node_id]])\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "#\n",
    "#         neighbour_id_list = []\n",
    "#         # neighbour_id_index = []\n",
    "#         for edge_index in neighbours_edge_index:\n",
    "#             neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "#             neighbour_edge = neighbour_edge_df.values[0]\n",
    "#             if neighbour_edge[0] != current_node_id:\n",
    "#                 neighbour_id = neighbour_edge[0]\n",
    "#             else:\n",
    "#                 neighbour_id = neighbour_edge[1]\n",
    "#             neighbour_id_list.append([neighbour_id])\n",
    "#             # neighbour_id_index.append(0)\n",
    "#         #\n",
    "#         # # steps 自动-1 比如想要3步的话 就传2\n",
    "#         # neighbour_id_list = find_neighbours(0, neighbour_id_list, neighbour_id_index)  #,current_x,current_y,500)\n",
    "#         neighbour_id_weights = []\n",
    "#         # start at 36\n",
    "#         for neighbour_ids in neighbour_id_list:\n",
    "#             poi_weight = 0\n",
    "#             for neighbour_id in neighbour_ids:\n",
    "#                 neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "#                                                         torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "#                 poi_weight += torch.sum(neighbour_features)\n",
    "#             neighbour_id_weights.append(poi_weight)\n",
    "#\n",
    "#         # print(f\"neighbour_id_weights = {neighbour_id_weights}\")\n",
    "#         neighbour_id_weights = np.array(neighbour_id_weights)\n",
    "#         normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n",
    "#\n",
    "#         neighbour_weights_index = 0\n",
    "#\n",
    "#         if np.isnan(normalized_neighbour_weights).all():\n",
    "#             no_poi_nodes.add(current_node_id)\n",
    "#             if len(neighbours_edge_index) == 0:\n",
    "#                 current_node_seq.append(current_node_id)\n",
    "#                 pos_node_seq.append(current_node_seq)\n",
    "#                 if len(poi_nodes) == 0:\n",
    "#                     init_neg_node = street_nodes_df.sample()\n",
    "#                     # print(init_neg_node.index.values[0])\n",
    "#                     neg_node_seq.append(init_neg_node.index.values[0])\n",
    "#                 else:\n",
    "#                     neg_node_seq.append(np.random.choice(list(poi_nodes)))\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 neighbour_weights_index = np.random.choice(len(neighbours_edge_index))\n",
    "#         else:\n",
    "#             poi_nodes.add(current_node_id)\n",
    "#             neighbour_weights_index = np.random.choice(len(normalized_neighbour_weights),\n",
    "#                                                        p=normalized_neighbour_weights)\n",
    "#\n",
    "#         next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "#         next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "#         next_edge = next_edge_df.values[0]\n",
    "#         if next_edge[0] != current_node_id:\n",
    "#             current_node_id = next_edge[0]\n",
    "#         else:\n",
    "#             current_node_id = next_edge[1]\n",
    "#         current_node_seq.append(current_node_id)\n",
    "#         pos_node_seq.append(current_node_seq)\n",
    "#         if len(no_poi_nodes) == 0:\n",
    "#             init_neg_node = street_nodes_df.sample()\n",
    "#             # print(init_neg_node.index.values[0])\n",
    "#             neg_node_seq.append(init_neg_node.index.values[0])\n",
    "#         else:\n",
    "#             neg_node_seq.append(np.random.choice(list(no_poi_nodes)))\n",
    "#     return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "#         np.asarray(neg_node_seq, dtype=np.int32))\n",
    "#\n",
    "#\n",
    "# # bfs like\n",
    "# def find_neighbours(steps, neighbour_id_list, neighbour_id_index_list):  #,origin_x,origin_y,max_dist):\n",
    "#     if steps <= 0:\n",
    "#         return neighbour_id_list\n",
    "#\n",
    "#     current = neighbour_id_list\n",
    "#     neighbour_id_index = []\n",
    "#     for i, neighbour_list in enumerate(current):\n",
    "#         short_neighbour_list = neighbour_list[neighbour_id_index_list[i]:]\n",
    "#         neighbour_id_index.append(len(neighbour_list) + 1)\n",
    "#         for neigh in short_neighbour_list:\n",
    "#             neighbours_edge_index = (street_edges_index_tensor == neigh).nonzero(as_tuple=True)[1]\n",
    "#\n",
    "#             for edge_index in neighbours_edge_index:\n",
    "#                 neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "#                 neighbour_edge = neighbour_edge_df.values[0]\n",
    "#                 if neighbour_edge[0] != neigh:\n",
    "#                     neighbour_id = neighbour_edge[0]\n",
    "#                 else:\n",
    "#                     neighbour_id = neighbour_edge[1]\n",
    "#\n",
    "#                 # 找距离？\n",
    "#                 # loc_x,loc_y = street_nodes_df.iloc[[neighbour_id]][\"x\"],street_nodes_df.iloc[[neighbour_id]][\"y\"]\n",
    "#                 # distance_to_origin = ox.distance.euclidean_dist_vec(loc_y,loc_x,\n",
    "#                 #                                                  origin_y,origin_x)\n",
    "#                 # if distance_to_origin > max_dist:\n",
    "#                 #     continue\n",
    "#\n",
    "#                 neighbour_id_list[i].append(neighbour_id)\n",
    "#     return find_neighbours(steps - 1, neighbour_id_list, neighbour_id_index)  #,origin_x,origin_y,max_dist)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# def custom_neg_sampling(\n",
    "#         edge_weight: Tensor,\n",
    "#         batch: Tensor,\n",
    "#         adj_size: int\n",
    "# ) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "#     neg_node_seq = []\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_id = start_node_id\n",
    "#         # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "#         neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "#         neighbour_weights_avg = np.average(neighbour_weights)\n",
    "#\n",
    "#         # 选出edge对应的weight\n",
    "#         negative_neighbour_weights_min = 0\n",
    "#         random_neg_index = torch.randint(0, 1, (1, 1), dtype=torch.long)\n",
    "#         while negative_neighbour_weights_min < neighbour_weights_avg:\n",
    "#             random_neg_index = torch.randint(0, adj_size, (1, 1), dtype=torch.long)\n",
    "#             negative_neighbours_edge_index = (street_edges_index_tensor == random_neg_index).nonzero(as_tuple=True)[1]\n",
    "#             negative_neighbour_weights = torch.index_select(edge_weight, 0, negative_neighbours_edge_index)\n",
    "#             negative_neighbour_weights_min = min(negative_neighbour_weights)\n",
    "#         neg_node_seq.append(random_neg_index.item())\n",
    "#     return torch.from_numpy(np.array(neg_node_seq, dtype=np.compat.long))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RawNeighborSampler This module iteratively samples neighbors (at each layer) and constructs bipartite graphs that simulate the actual computation flow of GNNs.\n",
    "\n",
    "format-selected)\n",
    "NeighborSampler holds the current :obj:batch_size, the IDs :obj:n_id of all nodes involved in the computation, and a list of bipartite graph objects via the tuple :obj:(edge_index, e_id, size), where :obj:edge_index represents the bipartite edges between source and target nodes, :obj:e_id denotes the IDs of original edges in the full graph, and :obj:size holds the shape of the bipartite graph.\n",
    "\n",
    "The actual computation graphs are then returned in reverse-mode, meaning that we pass messages from a larger set of nodes to a smaller one, until we reach the nodes for which we originally wanted to compute embeddings.\n",
    "https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "https://towardsdatascience.com/pytorch-geometric-graph-embedding-da71d614c3a\n",
    "https://gist.github.com/anuradhawick/904e7f2d2101f4b76516d04046007426\n",
    "https://zhuanlan.zhihu.com/p/387262710\n",
    "\"\"\"\n",
    "\n",
    "class NeighborSampler(RawNeighborSampler):\n",
    "    def sample(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "\n",
    "        pos_batch, neg_batch = custom_pos_sampling_with_POI(street_edges_weight_tensor, batch)\n",
    "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
    "        return super(NeighborSampler,self).sample(batch)\n",
    "\n",
    "train_loader = NeighborSampler(street_edges_index_tensor, sizes=NEIGHBOUR_SIZE, batch_size=1024, num_nodes=number_of_nodes,\n",
    "                                num_workers=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            # print(f\"x_target {x_target}\")\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SAGE(number_of_node_features, hidden_channels=HIDDEN_LAYER, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x, edge_index = street_nodes_features_tensor.to(device), street_edges_index_tensor.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def regression_train(embedding_df):\n",
    "    AKL_df = pd.read_csv(f\"{output_path}/datasets/property_data_with_street.csv\", encoding='latin1')\n",
    "    AKL_df = AKL_df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    akl_embedding_np = embedding_df.numpy()  #convert to Numpy array\n",
    "    akl_embedding_df = pd.DataFrame(akl_embedding_np)  #convert to a dataframe\n",
    "    embedding_size = akl_embedding_df.shape[1]\n",
    "    akl_embedding_df.columns = ['street_embedding_' + str(i) for i in range(embedding_size)]\n",
    "\n",
    "    akl_street_nodes_df = pd.read_csv(f\"{output_path}/datasets/akl_street_nodes.csv\")\n",
    "    akl_street_nodes_df = akl_street_nodes_df.rename(columns={\"source\": \"street_sources\", \"target\": \"street_targets\"})\n",
    "    AKL_df = find_embedding_for_property(AKL_df, akl_street_nodes_df, akl_embedding_df)\n",
    "    property_columns = ['CL_Suburb', 'CL_Sale_Tenure', 'CL_Sale_Date', 'CL_Land_Valuation_Capital_Value',\n",
    "                        'CL_Building_Floor_Area', 'CL_Building_Site_Cover',\n",
    "                        'CL_Land_Area', 'CL_Bldg_Const', 'CL_Bldg_Cond', 'CL_Roof_Const', 'CL_Roof_Cond',\n",
    "                        'CL_Category', 'CL_LUD_Age', 'CL_LUD_Land_Use_Description',\n",
    "                        'CL_MAS_No_Main_Roof_Garages', 'CL_Bedrooms', 'CL_Bathrooms'] + ['street_embedding_' + str(i)\n",
    "                                                                                         for i in range(embedding_size)]\n",
    "    X_columns = AKL_df[property_columns].values\n",
    "    #print(X_columns)\n",
    "    Y_column = AKL_df['Log_Sale_Price_Net'].values\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_columns, Y_column, test_size=0.2, random_state=1,\n",
    "                                                        shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1, shuffle=True)\n",
    "    hedonic_regression = LinearRegression()\n",
    "    hedonic_regression.fit(X_train, Y_train)\n",
    "\n",
    "    hedonic_regression_validation_result = hedonic_regression.predict(X_val)\n",
    "\n",
    "    validation_RMSE = round(mean_squared_error(Y_val, hedonic_regression_validation_result), 4)\n",
    "    validation_R2 = round(r2_score(Y_val, hedonic_regression_validation_result), 4)\n",
    "    return validation_RMSE, validation_R2\n",
    "\n",
    "\n",
    "def find_embedding_for_property(property_df, street_df, emb_df):\n",
    "    street_with_embedding = street_df.merge(emb_df, left_index=True, right_index=True)\n",
    "    output_df = property_df.merge(street_with_embedding, on=[\"street_sources\", \"street_targets\"])\n",
    "    return output_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7982,Current RMSE is 0.0597, Current R2 is 0.7973 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.7653,Current RMSE is 0.0604, Current R2 is 0.7951 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.7628,Current RMSE is 0.0605, Current R2 is 0.7949 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.7565,Current RMSE is 0.0602, Current R2 is 0.7956 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_2532/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.7579, Stopped on r2! Current R2 is 0.7955, previous R2 is 0.7956 \n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    # i=0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # i+=1\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x[n_id].to(device), adjs)\n",
    "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "        # print(i)\n",
    "    return total_loss / number_of_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_model_embedding():\n",
    "    model.eval()\n",
    "    embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "historical_rmse, historical_r2, historical_loss = np.inf, np.inf, np.inf\n",
    "for epoch in range(1, 20):\n",
    "    loss = train()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        temp_embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "        regression_rmse, regression_r2 = regression_train(temp_embedding)\n",
    "\n",
    "        if historical_rmse == np.inf:\n",
    "            historical_rmse = regression_rmse\n",
    "\n",
    "        if historical_r2 == np.inf:\n",
    "            historical_r2 = regression_r2\n",
    "\n",
    "        if historical_loss == np.inf:\n",
    "            historical_loss = loss\n",
    "\n",
    "        if loss > historical_loss:\n",
    "            # if regression_rmse > historical_rmse:\n",
    "            #     print(\n",
    "            #         f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on RMSE! Current RMSE is {regression_rmse}, previous RMSE is {historical_rmse} ')\n",
    "            #     break\n",
    "\n",
    "            if regression_r2 < historical_r2:\n",
    "                print(\n",
    "                    f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on r2! Current R2 is {regression_r2}, previous R2 is {historical_r2} ')\n",
    "                break\n",
    "        else:\n",
    "            historical_loss = loss\n",
    "            historical_rmse = regression_rmse\n",
    "            historical_r2 = regression_r2\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},Current RMSE is {regression_rmse}, Current R2 is {regression_r2} ')\n",
    "        output_embedding = temp_embedding\n",
    "\n",
    "# output_embedding = get_model_embedding()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2508,  0.0092, -0.0485,  ...,  0.0133, -0.1574,  0.4054],\n",
      "        [-0.2508,  0.0092, -0.0485,  ...,  0.0133, -0.1574,  0.4054],\n",
      "        [-0.2508,  0.0092, -0.0485,  ...,  0.0133, -0.1574,  0.4054],\n",
      "        ...,\n",
      "        [-0.2508,  0.0092, -0.0485,  ...,  0.0133, -0.1574,  0.4054],\n",
      "        [-0.2508,  0.0092, -0.0485,  ...,  0.0133, -0.1574,  0.4054],\n",
      "        [-0.2508,  0.0092, -0.0485,  ...,  0.0133, -0.1574,  0.4054]])\n"
     ]
    }
   ],
   "source": [
    "print(output_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akl_embedding_1668582832.csv\n"
     ]
    }
   ],
   "source": [
    "output_np = output_embedding.numpy()  #convert to Numpy array\n",
    "output_df = pd.DataFrame(output_np)  #convert to a dataframe\n",
    "current_GMT = time.gmtime()\n",
    "ts = calendar.timegm(current_GMT)\n",
    "output_df.to_csv(f\"./outputs/akl_embedding_{ts}.csv\", index=False)  #save to file\n",
    "print(f\"akl_embedding_{ts}.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
