{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import calendar\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler as RawNeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# output_path = \"/run/media/yunchen/lacie\"\n",
    "#dataset_path = \"./datasets/prewalk/with_amenity_filters\"\n",
    "dataset_path = \"./datasets/without_amenity_filters\"\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "HIDDEN_LAYER = 16\n",
    "NUM_LAYERS = 5\n",
    "NEIGHBOUR_SIZE = [5,5,5,3,3]\n",
    "DROP_OUT = 0.3\n",
    "BATCH_SIZE=1024\n",
    "NUM_WORKERS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,      2,      0,  ..., 458250, 458250, 458251],\n",
      "        [     2,      0,      1,  ..., 458249, 458251, 458250]])\n",
      "tensor([0.0168, 0.0168, 0.0175,  ..., 0.0571, 0.0857, 0.0857])\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = pd.read_csv(f\"{dataset_path}/akl_prewalked_nodes.csv\")\n",
    "street_edges_df = pd.read_csv(f\"{dataset_path}/akl_street_edges.csv\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "street_edges_df[['distance']] = scaler.fit_transform(street_edges_df[['distance']])\n",
    "\n",
    "source_street_index, targe_street_index, street_distance_weight = street_edges_df[\"source_street\"], street_edges_df[\n",
    "    \"target_street\"], street_edges_df[\"distance\"]\n",
    "\n",
    "# isolated_list = []\n",
    "# non_isolated_source = set(source_street_index.values.tolist())\n",
    "# non_isolated_target = set(targe_street_index.values.tolist())\n",
    "# for isolated_i,_ in street_nodes_df.iterrows():\n",
    "#     if isolated_i not in non_isolated_source and isolated_i  not in non_isolated_target:\n",
    "#         isolated_list.append(isolated_i)\n",
    "# street_nodes_df.drop(axis=0,index = isolated_list,inplace=True)\n",
    "\n",
    "street_edges_source_index_tensor = torch.tensor([source_street_index.values.tolist()])\n",
    "street_edges_target_index_tensor = torch.tensor([targe_street_index.values.tolist()])\n",
    "street_edges_index_tensor = torch.cat((street_edges_source_index_tensor, street_edges_target_index_tensor), 0)\n",
    "street_edges_weight_tensor = torch.tensor(street_distance_weight.values.tolist())\n",
    "\n",
    "print(street_edges_index_tensor)\n",
    "print(street_edges_weight_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Positive POI 16148  Negative POI 442104\n",
      "Layer 1 Positive POI 30711  Negative POI 427541\n",
      "Layer 2 Positive POI 52050  Negative POI 406202\n",
      "Layer 3 Positive POI 70259  Negative POI 387993\n",
      "Layer 4 Positive POI 86345  Negative POI 371907\n",
      "Layer 5 Positive POI 100598  Negative POI 357654\n"
     ]
    }
   ],
   "source": [
    "count_poi_df = street_nodes_df.copy()\n",
    "# count_poi_df[\"poi_count\"] = count_poi_df.apply(lambda row:row.amenity+row.restaurant+row.education+row.healthcare+row.shop+row.cloth,axis=1)\n",
    "count_poi_df[\"poi_count\"] = count_poi_df.apply(lambda row:row.amenity+row.restaurant+row.school+row.healthcare+row.shop+row.clothes,axis=1)\n",
    "positive_nodes_with_poi_df_layer0 = count_poi_df[count_poi_df[\"poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer0 = count_poi_df[count_poi_df[\"poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer1 = count_poi_df[count_poi_df[\"Layer_1_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer1 = count_poi_df[count_poi_df[\"Layer_1_agg_poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer2 = count_poi_df[count_poi_df[\"Layer_2_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer2 = count_poi_df[count_poi_df[\"Layer_2_agg_poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer3 = count_poi_df[count_poi_df[\"Layer_3_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer3 = count_poi_df[count_poi_df[\"Layer_3_agg_poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer4 = count_poi_df[count_poi_df[\"Layer_4_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer4 = count_poi_df[count_poi_df[\"Layer_4_agg_poi_count\"]<=0]\n",
    "positive_nodes_with_poi_df_layer5 = count_poi_df[count_poi_df[\"Layer_5_agg_poi_count\"]>0]\n",
    "negative_nodes_without_poi_df_layer5 = count_poi_df[count_poi_df[\"Layer_5_agg_poi_count\"]<=0]\n",
    "print(f\"Layer 0 Positive POI {len(positive_nodes_with_poi_df_layer0)}  Negative POI {len(negative_nodes_without_poi_df_layer0)}\")\n",
    "print(f\"Layer 1 Positive POI {len(positive_nodes_with_poi_df_layer1)}  Negative POI {len(negative_nodes_without_poi_df_layer1)}\")\n",
    "print(f\"Layer 2 Positive POI {len(positive_nodes_with_poi_df_layer2)}  Negative POI {len(negative_nodes_without_poi_df_layer2)}\")\n",
    "print(f\"Layer 3 Positive POI {len(positive_nodes_with_poi_df_layer3)}  Negative POI {len(negative_nodes_without_poi_df_layer3)}\")\n",
    "print(f\"Layer 4 Positive POI {len(positive_nodes_with_poi_df_layer4)}  Negative POI {len(negative_nodes_without_poi_df_layer4)}\")\n",
    "print(f\"Layer 5 Positive POI {len(positive_nodes_with_poi_df_layer5)}  Negative POI {len(negative_nodes_without_poi_df_layer5)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['restaurant', 'amenity', 'school', 'healthcare', 'shop', 'clothes',\n",
      "       'Average_POI_Distance', 'Layer_1_agg_restaurant', 'Layer_1_agg_amenity',\n",
      "       'Layer_1_agg_school', 'Layer_1_agg_healthcare', 'Layer_1_agg_shop',\n",
      "       'Layer_1_agg_clothes', 'Layer_1_agg_average_poi_distance',\n",
      "       'Layer_2_agg_restaurant', 'Layer_2_agg_amenity', 'Layer_2_agg_school',\n",
      "       'Layer_2_agg_healthcare', 'Layer_2_agg_shop', 'Layer_2_agg_clothes',\n",
      "       'Layer_2_agg_average_poi_distance', 'Layer_3_agg_restaurant',\n",
      "       'Layer_3_agg_amenity', 'Layer_3_agg_school', 'Layer_3_agg_healthcare',\n",
      "       'Layer_3_agg_shop', 'Layer_3_agg_clothes',\n",
      "       'Layer_3_agg_average_poi_distance', 'Layer_4_agg_restaurant',\n",
      "       'Layer_4_agg_amenity', 'Layer_4_agg_school', 'Layer_4_agg_healthcare',\n",
      "       'Layer_4_agg_shop', 'Layer_4_agg_clothes',\n",
      "       'Layer_4_agg_average_poi_distance', 'Layer_5_agg_restaurant',\n",
      "       'Layer_5_agg_amenity', 'Layer_5_agg_school', 'Layer_5_agg_healthcare',\n",
      "       'Layer_5_agg_shop', 'Layer_5_agg_clothes',\n",
      "       'Layer_5_agg_average_poi_distance'],\n",
      "      dtype='object')\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0207],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0136],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0236],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239]],\n",
      "       dtype=torch.float64)\n",
      "458252\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df_copy = street_nodes_df.copy()\n",
    "# street_nodes_df_copy = street_nodes_df_copy[[\"Layer_5_agg_restaurant\", \"Layer_5_agg_amenity\",\"Layer_5_agg_education\",\"Layer_5_agg_healthcare\",\"Layer_5_agg_shop\",\"Layer_5_agg_cloth\",\"Layer_5_agg_average_poi_distance\"]]\n",
    "# street_nodes_df_copy = street_nodes_df_copy[[\"restaurant\",\"amenity\",\"education\",\"healthcare\",\"shop\",\"cloth\",\"Average_POI_Distance\",\"Layer_1_agg_restaurant\", \"Layer_1_agg_amenity\",\"Layer_1_agg_education\",\"Layer_1_agg_healthcare\",\"Layer_1_agg_shop\",\"Layer_1_agg_cloth\",\"Layer_1_agg_average_poi_distance\",\"Layer_2_agg_restaurant\", \"Layer_2_agg_amenity\",\"Layer_2_agg_education\",\"Layer_2_agg_healthcare\",\"Layer_2_agg_shop\",\"Layer_2_agg_cloth\",\"Layer_2_agg_average_poi_distance\",\"Layer_3_agg_restaurant\", \"Layer_3_agg_amenity\",\"Layer_3_agg_education\",\"Layer_3_agg_healthcare\",\"Layer_3_agg_shop\",\"Layer_3_agg_cloth\",\"Layer_3_agg_average_poi_distance\",\"Layer_4_agg_restaurant\", \"Layer_4_agg_amenity\",\"Layer_4_agg_education\",\"Layer_4_agg_healthcare\",\"Layer_4_agg_shop\",\"Layer_4_agg_cloth\",\"Layer_4_agg_average_poi_distance\",\"Layer_5_agg_restaurant\", \"Layer_5_agg_amenity\",\"Layer_5_agg_education\",\"Layer_5_agg_healthcare\",\"Layer_5_agg_shop\",\"Layer_5_agg_cloth\",\"Layer_5_agg_average_poi_distance\",]]\n",
    "\n",
    "#street_nodes_df_copy = street_nodes_df_copy[[\"restaurant\", \"amenity\",\"school\",\"healthcare\",\"shop\",\"clothes\"]]\n",
    "#street_nodes_df_copy = street_nodes_df_copy[[\"Layer_5_agg_restaurant\", \"Layer_5_agg_amenity\",\"Layer_5_agg_school\",\"Layer_5_agg_healthcare\",\"Layer_5_agg_shop\",\"Layer_5_agg_clothes\"]]#,\"Layer_5_agg_average_poi_distance\"]]\n",
    "street_nodes_df_copy = street_nodes_df_copy[[\"restaurant\",\"amenity\",\"school\",\"healthcare\",\"shop\",\"clothes\",\"Average_POI_Distance\",\"Layer_1_agg_restaurant\", \"Layer_1_agg_amenity\",\"Layer_1_agg_school\",\"Layer_1_agg_healthcare\",\"Layer_1_agg_shop\",\"Layer_1_agg_clothes\",\"Layer_1_agg_average_poi_distance\",\"Layer_2_agg_restaurant\", \"Layer_2_agg_amenity\",\"Layer_2_agg_school\",\"Layer_2_agg_healthcare\",\"Layer_2_agg_shop\",\"Layer_2_agg_clothes\",\"Layer_2_agg_average_poi_distance\",\"Layer_3_agg_restaurant\", \"Layer_3_agg_amenity\",\"Layer_3_agg_school\",\"Layer_3_agg_healthcare\",\"Layer_3_agg_shop\",\"Layer_3_agg_clothes\",\"Layer_3_agg_average_poi_distance\",\"Layer_4_agg_restaurant\", \"Layer_4_agg_amenity\",\"Layer_4_agg_school\",\"Layer_4_agg_healthcare\",\"Layer_4_agg_shop\",\"Layer_4_agg_clothes\",\"Layer_4_agg_average_poi_distance\",\"Layer_5_agg_restaurant\", \"Layer_5_agg_amenity\",\"Layer_5_agg_school\",\"Layer_5_agg_healthcare\",\"Layer_5_agg_shop\",\"Layer_5_agg_clothes\",\"Layer_5_agg_average_poi_distance\",]]\n",
    "print(street_nodes_df_copy.columns)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "street_nodes_df_transformed = scaler.fit_transform(street_nodes_df_copy)\n",
    "street_nodes_features_tensor = torch.tensor(street_nodes_df_transformed)\n",
    "\n",
    "number_of_nodes = len(street_nodes_features_tensor)\n",
    "number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "print(street_nodes_features_tensor)\n",
    "print(number_of_nodes)\n",
    "print(number_of_node_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# street_nodes_df = street_nodes_df[street_nodes_df.columns[4:]]\n",
    "#\n",
    "# street_nodes_df_copy = street_nodes_df.copy()\n",
    "# print(street_nodes_df_copy.head)\n",
    "# # street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\",\"x\",\"y\"], axis=1, inplace=True)\n",
    "# street_nodes_df_copy.drop([\"street_length\",\"Average_POI_Distance\"], axis=1, inplace=True)\n",
    "# print(street_nodes_df_copy.columns)\n",
    "#\n",
    "# street_nodes_features_tensor = torch.tensor(street_nodes_df_copy.values.tolist())\n",
    "#\n",
    "# number_of_nodes = len(street_nodes_features_tensor)\n",
    "# number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "# print(street_nodes_features_tensor)\n",
    "# print(number_of_nodes)\n",
    "# print(number_of_node_features)\n",
    "#\n",
    "# street_nodes_df_copy_2 = street_nodes_df.copy()\n",
    "# # street_nodes_df_copy_2[\"poi_count\"] = street_nodes_df_copy_2.apply(lambda row:row.amenity+row.restaurant+row.school+row.shop+row.healthcare+row.clothes,axis=1)\n",
    "# street_nodes_df_copy_2[\"poi_count\"] = street_nodes_df_copy_2.apply(lambda row:row.amenity+row.restaurant+row.education+row.healthcare+row.shop+row.cloth,axis=1)\n",
    "# positive_nodes_with_poi_df_layer = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]>0]\n",
    "# negative_nodes_without_poi_df_layer = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]<=0]\n",
    "# positive_nodes_index = torch.tensor(positive_nodes_with_poi_df.index)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# def custom_pos_sampling_with_POI(\n",
    "#         edge_weight: Tensor,\n",
    "#         batch: Tensor,\n",
    "# ):\n",
    "#     pos_node_seq = []\n",
    "#     neg_node_seq = []\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_seq = [start_node_id.item()]\n",
    "#         current_node_id = current_node_seq[-1]\n",
    "#\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "#         len_neighbours_edge_index = len(neighbours_edge_index)\n",
    "#         neighbour_id_list = []\n",
    "#         for i in neighbours_edge_index:\n",
    "#             neighbour_id_list.append(select_neighbour_node_id_from_edge(i,current_node_id))\n",
    "#\n",
    "#         normalized_neighbour_poi_weights = calculate_normalized_poi_weights(neighbour_id_list)\n",
    "#\n",
    "#         global_node_without_poi = negative_nodes_without_poi_df.sample(replace=True).index.values[0]\n",
    "#         global_node_with_poi = positive_nodes_with_poi_df.sample(n=1,replace=True).index.values[0]\n",
    "#         # all neighbours don't have POI\n",
    "#         if np.isnan(normalized_neighbour_poi_weights).all():\n",
    "#             # Also no neighbour\n",
    "#             if len_neighbours_edge_index == 0:\n",
    "#                 pos_node_seq.append([current_node_id,global_node_without_poi])\n",
    "#                 neg_node_seq.append(global_node_with_poi)\n",
    "#                 # neg_node_seq.append(current_node_id)\n",
    "#                 # pos_node_seq.append([global_node_with_poi,global_node_with_poi])\n",
    "#                 continue\n",
    "#             # No neighbour has POIs but do have neighbours\n",
    "#             else:\n",
    "#                 # Pick a neighbour randomly using distance distribution to give future poi opportunity\n",
    "#                 neighbour_distance_weights = torch.index_select(edge_weight, 0, neighbours_edge_index).numpy()\n",
    "#                 norm_neighbour_distance_weights = calculate_normalized_distance_weights(neighbour_distance_weights)\n",
    "#                 neighbour_weights_index = np.random.choice(len(norm_neighbour_distance_weights),p=norm_neighbour_distance_weights)\n",
    "#\n",
    "#                 pos_node_seq.append([current_node_id,global_node_without_poi])\n",
    "#                 neg_node_seq.append(global_node_with_poi)\n",
    "#                 continue\n",
    "#         # normal case pick with poi distribution\n",
    "#         else:\n",
    "#             neighbour_weights_index = np.random.choice(len(normalized_neighbour_poi_weights),\n",
    "#                                                        p=normalized_neighbour_poi_weights)\n",
    "#\n",
    "#         next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "#         next_node_id = select_neighbour_node_id_from_edge(next_edge_index,current_node_id)\n",
    "#\n",
    "#         pos_node_seq.append([current_node_id,next_node_id])\n",
    "#         neg_node_seq.append(global_node_without_poi)\n",
    "#\n",
    "#     return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "#         np.asarray(neg_node_seq, dtype=np.int32))\n",
    "#\n",
    "# def calculate_normalized_distance_weights(neighbour_distance_weights):\n",
    "#     neighbour_distance_weights_sum = sum(neighbour_distance_weights)\n",
    "#     reverted_norm_neighbour_distance_weights = [1-(i / neighbour_distance_weights_sum) for i in neighbour_distance_weights]\n",
    "#\n",
    "#     reverted_norm_neighbour_distance_weights_sum = sum(reverted_norm_neighbour_distance_weights)\n",
    "#     norm_neighbour_distance_weights = [(i / reverted_norm_neighbour_distance_weights_sum) for i in reverted_norm_neighbour_distance_weights]\n",
    "#     return norm_neighbour_distance_weights\n",
    "#\n",
    "# def calculate_normalized_poi_weights(neighbour_id_list):\n",
    "#     neighbour_id_weights = []\n",
    "#     for neighbour_id in neighbour_id_list:\n",
    "#         poi_weight = 0\n",
    "#         neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "#                                                 torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "#         poi_weight += torch.sum(neighbour_features)\n",
    "#         neighbour_id_weights.append(poi_weight)\n",
    "#\n",
    "#     neighbour_poi_weights = np.array(neighbour_id_weights)\n",
    "#     neighbour_poi_weights_sum=sum(neighbour_poi_weights)\n",
    "#     normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
    "#     return normalized_neighbour_poi_weights\n",
    "#\n",
    "# def select_neighbour_node_id_from_edge(next_edge_index,current_node_id):\n",
    "#     next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "#     next_edge = next_edge_df.values[0]\n",
    "#     if next_edge[0] != current_node_id:\n",
    "#         neighbour_node_id = next_edge[0]\n",
    "#     else:\n",
    "#         neighbour_node_id = next_edge[1]\n",
    "#\n",
    "#     return neighbour_node_id\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def custom_sampling_with_Prewalk(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    "):\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        current_node_id = current_node_seq[-1]\n",
    "\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "        neighbour_id_list = []\n",
    "        for i in neighbours_edge_index:\n",
    "            neighbour_id_list.append(select_neighbour_node_id_from_edge(i,current_node_id))\n",
    "\n",
    "        normalized_neighbour_poi_weights = calculate_normalized_poi_weights(neighbour_id_list)\n",
    "\n",
    "        global_node_without_poi_layer5 = negative_nodes_without_poi_df_layer0.sample(replace=True).index.values[0]\n",
    "        global_node_with_poi_layer5 = positive_nodes_with_poi_df_layer0.sample(replace=True).index.values[0]\n",
    "        # all neighbours don't have POI\n",
    "        if np.isnan(normalized_neighbour_poi_weights).all():\n",
    "            pos_node_seq.append([current_node_id,global_node_without_poi_layer5])\n",
    "            neg_node_seq.append(global_node_with_poi_layer5)\n",
    "            continue\n",
    "        # normal case pick with poi distribution\n",
    "        else:\n",
    "            neighbour_weights_index = np.random.choice(len(normalized_neighbour_poi_weights),\n",
    "                                                       p=normalized_neighbour_poi_weights)\n",
    "\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_node_id = select_neighbour_node_id_from_edge(next_edge_index,current_node_id)\n",
    "\n",
    "        pos_node_seq.append([current_node_id,next_node_id])\n",
    "        neg_node_seq.append(global_node_without_poi_layer5)\n",
    "\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "        np.asarray(neg_node_seq, dtype=np.int32))\n",
    "\n",
    "def calculate_normalized_poi_weights(neighbour_id_list):\n",
    "    neighbour_id_weights = []\n",
    "    for neighbour_id in neighbour_id_list:\n",
    "        poi_weight = 0\n",
    "        neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "                                                torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "        neighbour_features = neighbour_features[0][len(neighbour_features)-6:]\n",
    "\n",
    "        poi_weight += torch.sum(neighbour_features)\n",
    "        neighbour_id_weights.append(poi_weight)\n",
    "\n",
    "    neighbour_poi_weights = np.array(neighbour_id_weights)\n",
    "    neighbour_poi_weights_sum=sum(neighbour_poi_weights)\n",
    "    normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
    "    return normalized_neighbour_poi_weights\n",
    "\n",
    "def select_neighbour_node_id_from_edge(next_edge_index,current_node_id):\n",
    "    next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "    next_edge = next_edge_df.values[0]\n",
    "    if next_edge[0] != current_node_id:\n",
    "        neighbour_node_id = next_edge[0]\n",
    "    else:\n",
    "        neighbour_node_id = next_edge[1]\n",
    "\n",
    "    return neighbour_node_id\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RawNeighborSampler This module iteratively samples neighbors (at each layer) and constructs bipartite graphs that simulate the actual computation flow of GNNs.\n",
    "\n",
    "format-selected)\n",
    "NeighborSampler holds the current :obj:batch_size, the IDs :obj:n_id of all nodes involved in the computation, and a list of bipartite graph objects via the tuple :obj:(edge_index, e_id, size), where :obj:edge_index represents the bipartite edges between source and target nodes, :obj:e_id denotes the IDs of original edges in the full graph, and :obj:size holds the shape of the bipartite graph.\n",
    "\n",
    "The actual computation graphs are then returned in reverse-mode, meaning that we pass messages from a larger set of nodes to a smaller one, until we reach the nodes for which we originally wanted to compute embeddings.\n",
    "https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "https://towardsdatascience.com/pytorch-geometric-graph-embedding-da71d614c3a\n",
    "https://gist.github.com/anuradhawick/904e7f2d2101f4b76516d04046007426\n",
    "https://zhuanlan.zhihu.com/p/387262710\n",
    "\"\"\"\n",
    "\n",
    "class NeighborSampler(RawNeighborSampler):\n",
    "    def sample(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "\n",
    "        pos_batch, neg_batch = custom_sampling_with_Prewalk(street_edges_weight_tensor, batch)\n",
    "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
    "        return super(NeighborSampler,self).sample(batch)\n",
    "\n",
    "train_loader = NeighborSampler(street_edges_index_tensor, sizes=NEIGHBOUR_SIZE, batch_size=BATCH_SIZE, num_nodes=number_of_nodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            # print(f\"x_target {x_target}\")\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SAGE(number_of_node_features, hidden_channels=HIDDEN_LAYER, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x, edge_index = street_nodes_features_tensor.to(device), street_edges_index_tensor.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def regression_train(embedding_df):\n",
    "    AKL_df = pd.read_csv(f\"{dataset_path}/property_data_with_street.csv\", encoding='latin1')\n",
    "    AKL_df = AKL_df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    akl_embedding_np = embedding_df.numpy()  #convert to Numpy array\n",
    "    akl_embedding_df = pd.DataFrame(akl_embedding_np)  #convert to a dataframe\n",
    "    embedding_size = akl_embedding_df.shape[1]\n",
    "    akl_embedding_df.columns = ['street_embedding_' + str(i) for i in range(embedding_size)]\n",
    "\n",
    "    akl_street_nodes_df = pd.read_csv(f\"{dataset_path}/akl_prewalked_nodes.csv\")\n",
    "    akl_street_nodes_df = akl_street_nodes_df.rename(columns={\"source\": \"street_sources\", \"target\": \"street_targets\"})\n",
    "    AKL_df = find_embedding_for_property(AKL_df, akl_street_nodes_df, akl_embedding_df)\n",
    "    property_columns = ['CL_Suburb', 'CL_Sale_Tenure', 'CL_Sale_Date', 'CL_Land_Valuation_Capital_Value',\n",
    "                        'CL_Building_Floor_Area', 'CL_Building_Site_Cover',\n",
    "                        'CL_Land_Area', 'CL_Bldg_Const', 'CL_Bldg_Cond', 'CL_Roof_Const', 'CL_Roof_Cond',\n",
    "                        'CL_Category', 'CL_LUD_Age', 'CL_LUD_Land_Use_Description',\n",
    "                        'CL_MAS_No_Main_Roof_Garages', 'CL_Bedrooms', 'CL_Bathrooms'] + ['street_embedding_' + str(i)\n",
    "                                                                                         for i in range(embedding_size)]\n",
    "    X_columns = AKL_df[property_columns].values\n",
    "    #print(X_columns)\n",
    "    Y_column = AKL_df['Log_Sale_Price_Net'].values\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_columns, Y_column, test_size=0.2, random_state=1,\n",
    "                                                        shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1, shuffle=True)\n",
    "    hedonic_regression = LinearRegression()\n",
    "    hedonic_regression.fit(X_train, Y_train)\n",
    "\n",
    "    hedonic_regression_validation_result = hedonic_regression.predict(X_val)\n",
    "\n",
    "    validation_RMSE = round(mean_squared_error(Y_val, hedonic_regression_validation_result), 4)\n",
    "    validation_R2 = round(r2_score(Y_val, hedonic_regression_validation_result), 4)\n",
    "    return validation_RMSE, validation_R2\n",
    "\n",
    "\n",
    "def find_embedding_for_property(property_df, street_df, emb_df):\n",
    "    street_with_embedding = street_df.merge(emb_df, left_index=True, right_index=True)\n",
    "    output_df = property_df.merge(street_with_embedding, on=[\"street_sources\", \"street_targets\"])\n",
    "    return output_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.0683,Current RMSE is 0.0606, Current R2 is 0.7945 \n",
      "Epoch: 002, Loss: 0.9907,Current RMSE is 0.0606, Current R2 is 0.7945 \n",
      "Epoch: 003, Loss: 0.9700,Current RMSE is 0.0605, Current R2 is 0.7946 \n",
      "Epoch: 004, Loss: 0.9607,Current RMSE is 0.0605, Current R2 is 0.7946 \n",
      "Epoch: 005, Loss: 0.9571,Current RMSE is 0.0604, Current R2 is 0.7949 \n",
      "Epoch: 006, Loss: 0.9544,Current RMSE is 0.0606, Current R2 is 0.7945 \n",
      "Epoch: 007, Loss: 0.9519,Current RMSE is 0.0605, Current R2 is 0.7947 \n",
      "Epoch: 008, Loss: 0.9468,Current RMSE is 0.0606, Current R2 is 0.7945 \n",
      "Epoch: 009, Loss: 0.9458,Current RMSE is 0.0605, Current R2 is 0.7947 \n",
      "Epoch: 010, Loss: 0.9430,Current RMSE is 0.0606, Current R2 is 0.7945 \n",
      "Epoch: 011, Loss: 0.9435, Stopped on r2! Current R2 is 0.7942, previous R2 is 0.7945 \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [55], line 69\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m03d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,Current RMSE is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mregression_rmse\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Current R2 is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mregression_r2\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     67\u001B[0m         output_embedding \u001B[38;5;241m=\u001B[39m temp_embedding\n\u001B[0;32m---> 69\u001B[0m output_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mget_model_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [55], line 29\u001B[0m, in \u001B[0;36mget_model_embedding\u001B[0;34m()\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mno_grad()\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_model_embedding\u001B[39m():\n\u001B[1;32m     28\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m---> 29\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m embedding\n",
      "Cell \u001B[0;32mIn [53], line 22\u001B[0m, in \u001B[0;36mSAGE.full_forward\u001B[0;34m(self, x, edge_index)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfull_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index):\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvs):\n\u001B[0;32m---> 22\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     24\u001B[0m             x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mrelu()\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch_geometric/nn/conv/sage_conv.py:132\u001B[0m, in \u001B[0;36mSAGEConv.forward\u001B[0;34m(self, x, edge_index, size)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001B[39;00m\n\u001B[1;32m    131\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpropagate(edge_index, x\u001B[38;5;241m=\u001B[39mx, size\u001B[38;5;241m=\u001B[39msize)\n\u001B[0;32m--> 132\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlin_l\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m x_r \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_weight \u001B[38;5;129;01mand\u001B[39;00m x_r \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:118\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m        x (Tensor): The features.\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    # i=0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # i+=1\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x[n_id].float().to(device), adjs)\n",
    "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "        # print(i)\n",
    "    return total_loss / number_of_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_model_embedding():\n",
    "    model.eval()\n",
    "    embedding = model.full_forward(x.float().to(device), edge_index.to(device)).cpu()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "historical_rmse, historical_r2, historical_loss = np.inf, np.inf, np.inf\n",
    "for epoch in range(1, 35):\n",
    "    loss = train()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        temp_embedding = model.full_forward(x.float().to(device), edge_index.to(device)).cpu()\n",
    "        regression_rmse, regression_r2 = regression_train(temp_embedding)\n",
    "\n",
    "        if historical_rmse == np.inf:\n",
    "            historical_rmse = regression_rmse\n",
    "\n",
    "        if historical_r2 == np.inf:\n",
    "            historical_r2 = regression_r2\n",
    "\n",
    "        if historical_loss == np.inf:\n",
    "            historical_loss = loss\n",
    "\n",
    "        if loss > historical_loss:\n",
    "            # if regression_rmse > historical_rmse:\n",
    "            #     print(\n",
    "            #         f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on RMSE! Current RMSE is {regression_rmse}, previous RMSE is {historical_rmse} ')\n",
    "            #     break\n",
    "\n",
    "            if regression_r2 < historical_r2:\n",
    "                print(\n",
    "                    f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on r2! Current R2 is {regression_r2}, previous R2 is {historical_r2} ')\n",
    "                if epoch > 10:\n",
    "                    break\n",
    "        else:\n",
    "            historical_loss = loss\n",
    "            historical_rmse = regression_rmse\n",
    "            historical_r2 = regression_r2\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},Current RMSE is {regression_rmse}, Current R2 is {regression_r2} ')\n",
    "        output_embedding = temp_embedding\n",
    "\n",
    "output_embedding = get_model_embedding()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(output_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_np = output_embedding.numpy()  #convert to Numpy array\n",
    "output_df = pd.DataFrame(output_np)  #convert to a dataframe\n",
    "current_GMT = time.gmtime()\n",
    "ts = calendar.timegm(current_GMT)\n",
    "output_df.to_csv(f\"./outputs/embeddings/without_amenity_filters/akl_embedding_{ts}.csv\", index=False)  #save to file\n",
    "print(f\"akl_embedding_{ts}.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
