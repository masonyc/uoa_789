{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score\n",
    "from typing import Optional, Tuple, Union\n",
    "import calendar\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler as RawNeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# output_path = \"/run/media/yunchen/lacie\"\n",
    "output_path = \".\"\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "HIDDEN_LAYER = 16\n",
    "NUM_LAYERS = 5\n",
    "NEIGHBOUR_SIZE = [5, 5]\n",
    "DROP_OUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         street_length  restaurant  Average_POI_Distance  school  amenity  \\\n",
      "0              32.641         0.0                32.641     0.0      0.0   \n",
      "1              68.146         0.0                68.146     0.0      0.0   \n",
      "2              64.232         0.0                64.232     0.0      0.0   \n",
      "3              50.560         0.0                50.560     0.0      0.0   \n",
      "4              20.232         0.0                20.232     0.0      0.0   \n",
      "...               ...         ...                   ...     ...      ...   \n",
      "463092         25.652         0.0                25.652     0.0      0.0   \n",
      "463093        166.770         0.0               166.770     0.0      0.0   \n",
      "463094        143.628         0.0               143.628     0.0      0.0   \n",
      "463095        185.149         0.0               185.149     0.0      0.0   \n",
      "463096        308.379         0.0               308.379     0.0      0.0   \n",
      "\n",
      "        shop  healthcare  clothes  \n",
      "0        0.0         0.0      0.0  \n",
      "1        0.0         0.0      0.0  \n",
      "2        0.0         0.0      0.0  \n",
      "3        0.0         0.0      0.0  \n",
      "4        0.0         0.0      0.0  \n",
      "...      ...         ...      ...  \n",
      "463092   0.0         0.0      0.0  \n",
      "463093   0.0         0.0      0.0  \n",
      "463094   0.0         0.0      0.0  \n",
      "463095   0.0         0.0      0.0  \n",
      "463096   0.0         0.0      0.0  \n",
      "\n",
      "[463097 rows x 8 columns]>\n",
      "   restaurant  school  amenity  shop  healthcare  clothes\n",
      "0         0.0     0.0      0.0   0.0         0.0      0.0\n",
      "1         0.0     0.0      0.0   0.0         0.0      0.0\n",
      "2         0.0     0.0      0.0   0.0         0.0      0.0\n",
      "3         0.0     0.0      0.0   0.0         0.0      0.0\n",
      "4         0.0     0.0      0.0   0.0         0.0      0.0\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "463097\n",
      "6\n",
      "7051\n",
      "456046\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = pd.read_csv(f\"{output_path}/datasets/akl_street_nodes.csv\")\n",
    "street_nodes_df = street_nodes_df[street_nodes_df.columns[4:]]\n",
    "\n",
    "street_nodes_df_copy = street_nodes_df.copy()\n",
    "print(street_nodes_df_copy.head)\n",
    "street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\"], axis=1, inplace=True)\n",
    "print(street_nodes_df_copy.head())\n",
    "\n",
    "street_nodes_features_tensor = torch.tensor(street_nodes_df_copy.values.tolist())\n",
    "\n",
    "number_of_nodes = len(street_nodes_features_tensor)\n",
    "number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "print(street_nodes_features_tensor)\n",
    "print(number_of_nodes)\n",
    "print(number_of_node_features)\n",
    "\n",
    "street_nodes_df_copy_2 = street_nodes_df.copy()\n",
    "street_nodes_df_copy_2[\"poi_count\"] = street_nodes_df_copy_2.apply(lambda row:row.amenity+row.school+row.shop+row.healthcare+row.clothes,axis=1)\n",
    "positive_nodes_with_poi_df = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]>0]\n",
    "negative_nodes_without_poi_df = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]<=0]\n",
    "print(len(positive_nodes_with_poi_df))\n",
    "print(len(negative_nodes_without_poi_df))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,      2,      0,  ..., 463095, 463095, 463096],\n",
      "        [     2,      0,      1,  ..., 463094, 463096, 463095]])\n",
      "tensor([ 96.8730,  96.8730, 100.7870,  ..., 328.7770, 493.5280, 493.5280])\n"
     ]
    }
   ],
   "source": [
    "street_edges_df = pd.read_csv(f\"{output_path}/datasets/akl_street_edges.csv\")\n",
    "source_street_index, targe_street_index, street_distance_weight = street_edges_df[\"source_street\"], street_edges_df[\n",
    "    \"target_street\"], street_edges_df[\"distance\"]\n",
    "street_edges_source_index_tensor = torch.tensor([source_street_index.values.tolist()])\n",
    "street_edges_target_index_tensor = torch.tensor([targe_street_index.values.tolist()])\n",
    "street_edges_index_tensor = torch.cat((street_edges_source_index_tensor, street_edges_target_index_tensor), 0)\n",
    "street_edges_weight_tensor = torch.tensor(street_distance_weight.values.tolist())\n",
    "\n",
    "print(street_edges_index_tensor)\n",
    "print(street_edges_weight_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [],
   "source": [
    "def custom_pos_sampling_with_POI(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    "):\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        current_node_id = current_node_seq[-1]\n",
    "\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "        len_neighbours_edge_index = len(neighbours_edge_index)\n",
    "        neighbour_id_list = []\n",
    "        for i in neighbours_edge_index:\n",
    "            neighbour_id_list.append(select_neighbour_node_id_from_edge(i,current_node_id))\n",
    "\n",
    "        normalized_neighbour_poi_weights = calculate_normalized_poi_weights(neighbour_id_list,current_node_id)\n",
    "\n",
    "        # all neighbours don't have POI\n",
    "        if np.isnan(normalized_neighbour_poi_weights).all():\n",
    "            global_node_with_poi = positive_nodes_with_poi_df.sample(n=2,replace=True).index.values\n",
    "            # Also no neighbour\n",
    "            if len_neighbours_edge_index == 0:\n",
    "                neg_node_seq.append(current_node_id)\n",
    "                pos_node_seq.append([global_node_with_poi[0],global_node_with_poi[1]])\n",
    "                continue\n",
    "            # No neighbour has POIs but do have neighbours\n",
    "            else:\n",
    "                # Pick a neighbour randomly using distance distribution to give future poi opportunity\n",
    "                neighbour_distance_weights = torch.index_select(edge_weight, 0, neighbours_edge_index).numpy()\n",
    "                norm_neighbour_distance_weights = calculate_normalized_distance_weights(neighbour_distance_weights,current_node_id)\n",
    "                neighbour_weights_index = np.random.choice(len(norm_neighbour_distance_weights),p=norm_neighbour_distance_weights)\n",
    "        # normal case pick with poi distribution\n",
    "        else:\n",
    "            # TODO: use current node's majority poi type to select a different type of POI from neighbour\n",
    "            # probability transform\n",
    "            neighbour_weights_index = np.random.choice(len(normalized_neighbour_poi_weights),\n",
    "                                                       p=normalized_neighbour_poi_weights)\n",
    "\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_node_id = select_neighbour_node_id_from_edge(next_edge_index,current_node_id)\n",
    "\n",
    "        global_node_without_poi = negative_nodes_without_poi_df.sample(replace=True).index.values[0]\n",
    "\n",
    "        current_node_seq.append(next_node_id)\n",
    "        pos_node_seq.append(current_node_seq)\n",
    "        neg_node_seq.append(global_node_without_poi)\n",
    "\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "        np.asarray(neg_node_seq, dtype=np.int32))\n",
    "\n",
    "def get_sample(df,n):\n",
    "    return df[np.random.choice(df.shape[0], n, replace=True)]\n",
    "\n",
    "cached_neighbour_distance_weights_dict={}\n",
    "def calculate_normalized_distance_weights(neighbour_distance_weights,cache_key):\n",
    "    if cache_key in cached_neighbour_distance_weights_dict.keys():\n",
    "        return cached_neighbour_distance_weights_dict[cache_key]\n",
    "    neighbour_distance_weights_sum = sum(neighbour_distance_weights)\n",
    "    reverted_norm_neighbour_distance_weights = [1-(i / neighbour_distance_weights_sum) for i in neighbour_distance_weights]\n",
    "\n",
    "    reverted_norm_neighbour_distance_weights_sum = sum(reverted_norm_neighbour_distance_weights)\n",
    "    norm_neighbour_distance_weights = [(i / reverted_norm_neighbour_distance_weights_sum) for i in reverted_norm_neighbour_distance_weights]\n",
    "    cached_neighbour_distance_weights_dict[cache_key] = norm_neighbour_distance_weights\n",
    "    return norm_neighbour_distance_weights\n",
    "\n",
    "\n",
    "cached_neighbour_poi_weights_dict = {}\n",
    "def calculate_normalized_poi_weights(neighbour_id_list,cache_key):\n",
    "    if cache_key in cached_neighbour_poi_weights_dict.keys():\n",
    "        return cached_neighbour_poi_weights_dict[cache_key]\n",
    "\n",
    "    neighbour_id_weights = []\n",
    "    for neighbour_id in neighbour_id_list:\n",
    "        poi_weight = 0\n",
    "        neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "                                                torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "        poi_weight += torch.sum(neighbour_features)\n",
    "        neighbour_id_weights.append(poi_weight)\n",
    "\n",
    "    neighbour_poi_weights = np.array(neighbour_id_weights)\n",
    "    neighbour_poi_weights_sum=sum(neighbour_poi_weights)\n",
    "    normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
    "\n",
    "    cached_neighbour_poi_weights_dict[cache_key] = normalized_neighbour_poi_weights\n",
    "    return normalized_neighbour_poi_weights\n",
    "\n",
    "cached_neighbour_node_id_dict ={}\n",
    "def select_neighbour_node_id_from_edge(next_edge_index,current_node_id):\n",
    "    if (current_node_id,next_edge_index) in cached_neighbour_node_id_dict.keys():\n",
    "        return cached_neighbour_node_id_dict[(current_node_id,next_edge_index)]\n",
    "\n",
    "    next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "    next_edge = next_edge_df.values[0]\n",
    "    if next_edge[0] != current_node_id:\n",
    "        neighbour_node_id = next_edge[0]\n",
    "    else:\n",
    "        neighbour_node_id = next_edge[1]\n",
    "\n",
    "    cached_neighbour_node_id_dict[(current_node_id,next_edge_index)] = neighbour_node_id\n",
    "    return neighbour_node_id\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "outputs": [],
   "source": [
    "# def custom_pos_sampling(\n",
    "#         edge_weight: tensor,\n",
    "#         batch: tensor,\n",
    "# ) -> union[tensor, tuple[tensor, tensor]]:\n",
    "#     pos_node_seq = []\n",
    "#     neg_node_seq = []\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_seq = [start_node_id.item()]\n",
    "#         total_distance = 0\n",
    "#         current_node_id = start_node_id\n",
    "#         # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=true)[1]\n",
    "#\n",
    "#         # 选出edge对应的weight\n",
    "#         neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "#         norm_neighbour_weights = [i / sum(neighbour_weights.numpy()) for i in neighbour_weights.numpy()]\n",
    "#         #根据概率随机选一个\n",
    "#         #print(neighbours_edge_index,len(neighbour_weights))\n",
    "#         if len(neighbour_weights) == 0:\n",
    "#             current_node_seq.append(current_node_id)\n",
    "#             pos_node_seq.append(current_node_seq)\n",
    "#             #neg_node_seq.append(current_node_seq)\n",
    "#             continue\n",
    "#         neighbour_weights_index = np.random.choice(len(neighbour_weights), p=norm_neighbour_weights)\n",
    "#\n",
    "#         # print(\"current node id \\n\", current_node_id)\n",
    "#         #print(\"neighbour weights \\n\", neighbour_weights)\n",
    "#         #print(\"neighbour weights index  \\n\", neighbour_weights_index)\n",
    "#         #print(neighbour_weights.min(),neighbour_weights.argmin())\n",
    "#\n",
    "#         # 取最近的边\n",
    "#         # todo：加入别的策略，poi信息等\n",
    "#         next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "#         next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "#         #print(\"next edge \\n\", next_edge_df)\n",
    "#         next_edge = next_edge_df.values[0]\n",
    "#         total_distance += next_edge[2]\n",
    "#         # next_edge[0] = source street\n",
    "#         # next_edge[1] = target_street\n",
    "#         # next_edge[2] = distance\n",
    "#         if next_edge[0] != current_node_id:\n",
    "#             current_node_id = next_edge[0]\n",
    "#         else:\n",
    "#             current_node_id = next_edge[1]\n",
    "#         current_node_seq.append(current_node_id)\n",
    "#         pos_node_seq.append(current_node_seq)\n",
    "#     #if len(neg_node_seq) >0 :\n",
    "#     #print(\"isolated node: {number} {node_list}\".format(number = len(neg_node_seq),node_list = neg_node_seq))\n",
    "#     return torch.from_numpynumpy(np.asarray(pos_node_seq, dtype=np.int32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "outputs": [],
   "source": [
    "# def custom_pos_sampling_with_POI(\n",
    "#         edge_weight: Tensor,\n",
    "#         batch: Tensor,\n",
    "# ):\n",
    "#     pos_node_seq = []\n",
    "#     neg_node_seq = []\n",
    "#     poi_nodes = set()\n",
    "#     no_poi_nodes = set()\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_seq = [start_node_id.item()]\n",
    "#         current_node_id = current_node_seq[-1]\n",
    "#         # 找距离？\n",
    "#         # current_x,current_y = street_nodes_df.iloc[[start_node_id]][\"x\"],street_nodes_df.iloc[[start_node_id]][\"y\"]\n",
    "#         # print(start_node_id)\n",
    "#         # print(street_nodes_df.iloc[[start_node_id]])\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "#\n",
    "#         neighbour_id_list = []\n",
    "#         # neighbour_id_index = []\n",
    "#         for edge_index in neighbours_edge_index:\n",
    "#             neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "#             neighbour_edge = neighbour_edge_df.values[0]\n",
    "#             if neighbour_edge[0] != current_node_id:\n",
    "#                 neighbour_id = neighbour_edge[0]\n",
    "#             else:\n",
    "#                 neighbour_id = neighbour_edge[1]\n",
    "#             neighbour_id_list.append([neighbour_id])\n",
    "#             # neighbour_id_index.append(0)\n",
    "#         #\n",
    "#         # # steps 自动-1 比如想要3步的话 就传2\n",
    "#         # neighbour_id_list = find_neighbours(0, neighbour_id_list, neighbour_id_index)  #,current_x,current_y,500)\n",
    "#         neighbour_id_weights = []\n",
    "#         # start at 36\n",
    "#         for neighbour_ids in neighbour_id_list:\n",
    "#             poi_weight = 0\n",
    "#             for neighbour_id in neighbour_ids:\n",
    "#                 neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "#                                                         torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "#                 poi_weight += torch.sum(neighbour_features)\n",
    "#             neighbour_id_weights.append(poi_weight)\n",
    "#\n",
    "#         # print(f\"neighbour_id_weights = {neighbour_id_weights}\")\n",
    "#         neighbour_id_weights = np.array(neighbour_id_weights)\n",
    "#         normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n",
    "#\n",
    "#         neighbour_weights_index = 0\n",
    "#\n",
    "#         if np.isnan(normalized_neighbour_weights).all():\n",
    "#             no_poi_nodes.add(current_node_id)\n",
    "#             if len(neighbours_edge_index) == 0:\n",
    "#                 current_node_seq.append(current_node_id)\n",
    "#                 pos_node_seq.append(current_node_seq)\n",
    "#                 if len(poi_nodes) == 0:\n",
    "#                     init_neg_node = street_nodes_df.sample()\n",
    "#                     # print(init_neg_node.index.values[0])\n",
    "#                     neg_node_seq.append(init_neg_node.index.values[0])\n",
    "#                 else:\n",
    "#                     neg_node_seq.append(np.random.choice(list(poi_nodes)))\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 neighbour_weights_index = np.random.choice(len(neighbours_edge_index))\n",
    "#         else:\n",
    "#             poi_nodes.add(current_node_id)\n",
    "#             neighbour_weights_index = np.random.choice(len(normalized_neighbour_weights),\n",
    "#                                                        p=normalized_neighbour_weights)\n",
    "#\n",
    "#         next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "#         next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "#         next_edge = next_edge_df.values[0]\n",
    "#         if next_edge[0] != current_node_id:\n",
    "#             current_node_id = next_edge[0]\n",
    "#         else:\n",
    "#             current_node_id = next_edge[1]\n",
    "#         current_node_seq.append(current_node_id)\n",
    "#         pos_node_seq.append(current_node_seq)\n",
    "#         if len(no_poi_nodes) == 0:\n",
    "#             init_neg_node = street_nodes_df.sample()\n",
    "#             # print(init_neg_node.index.values[0])\n",
    "#             neg_node_seq.append(init_neg_node.index.values[0])\n",
    "#         else:\n",
    "#             neg_node_seq.append(np.random.choice(list(no_poi_nodes)))\n",
    "#     return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "#         np.asarray(neg_node_seq, dtype=np.int32))\n",
    "#\n",
    "#\n",
    "# # bfs like\n",
    "# def find_neighbours(steps, neighbour_id_list, neighbour_id_index_list):  #,origin_x,origin_y,max_dist):\n",
    "#     if steps <= 0:\n",
    "#         return neighbour_id_list\n",
    "#\n",
    "#     current = neighbour_id_list\n",
    "#     neighbour_id_index = []\n",
    "#     for i, neighbour_list in enumerate(current):\n",
    "#         short_neighbour_list = neighbour_list[neighbour_id_index_list[i]:]\n",
    "#         neighbour_id_index.append(len(neighbour_list) + 1)\n",
    "#         for neigh in short_neighbour_list:\n",
    "#             neighbours_edge_index = (street_edges_index_tensor == neigh).nonzero(as_tuple=True)[1]\n",
    "#\n",
    "#             for edge_index in neighbours_edge_index:\n",
    "#                 neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "#                 neighbour_edge = neighbour_edge_df.values[0]\n",
    "#                 if neighbour_edge[0] != neigh:\n",
    "#                     neighbour_id = neighbour_edge[0]\n",
    "#                 else:\n",
    "#                     neighbour_id = neighbour_edge[1]\n",
    "#\n",
    "#                 # 找距离？\n",
    "#                 # loc_x,loc_y = street_nodes_df.iloc[[neighbour_id]][\"x\"],street_nodes_df.iloc[[neighbour_id]][\"y\"]\n",
    "#                 # distance_to_origin = ox.distance.euclidean_dist_vec(loc_y,loc_x,\n",
    "#                 #                                                  origin_y,origin_x)\n",
    "#                 # if distance_to_origin > max_dist:\n",
    "#                 #     continue\n",
    "#\n",
    "#                 neighbour_id_list[i].append(neighbour_id)\n",
    "#     return find_neighbours(steps - 1, neighbour_id_list, neighbour_id_index)  #,origin_x,origin_y,max_dist)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "outputs": [],
   "source": [
    "# def custom_neg_sampling(\n",
    "#         edge_weight: Tensor,\n",
    "#         batch: Tensor,\n",
    "#         adj_size: int\n",
    "# ) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "#     neg_node_seq = []\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_id = start_node_id\n",
    "#         # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "#         neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "#         neighbour_weights_avg = np.average(neighbour_weights)\n",
    "#\n",
    "#         # 选出edge对应的weight\n",
    "#         negative_neighbour_weights_min = 0\n",
    "#         random_neg_index = torch.randint(0, 1, (1, 1), dtype=torch.long)\n",
    "#         while negative_neighbour_weights_min < neighbour_weights_avg:\n",
    "#             random_neg_index = torch.randint(0, adj_size, (1, 1), dtype=torch.long)\n",
    "#             negative_neighbours_edge_index = (street_edges_index_tensor == random_neg_index).nonzero(as_tuple=True)[1]\n",
    "#             negative_neighbour_weights = torch.index_select(edge_weight, 0, negative_neighbours_edge_index)\n",
    "#             negative_neighbour_weights_min = min(negative_neighbour_weights)\n",
    "#         neg_node_seq.append(random_neg_index.item())\n",
    "#     return torch.from_numpy(np.array(neg_node_seq, dtype=np.compat.long))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RawNeighborSampler This module iteratively samples neighbors (at each layer) and constructs bipartite graphs that simulate the actual computation flow of GNNs.\n",
    "\n",
    "format-selected)\n",
    "NeighborSampler holds the current :obj:batch_size, the IDs :obj:n_id of all nodes involved in the computation, and a list of bipartite graph objects via the tuple :obj:(edge_index, e_id, size), where :obj:edge_index represents the bipartite edges between source and target nodes, :obj:e_id denotes the IDs of original edges in the full graph, and :obj:size holds the shape of the bipartite graph.\n",
    "\n",
    "The actual computation graphs are then returned in reverse-mode, meaning that we pass messages from a larger set of nodes to a smaller one, until we reach the nodes for which we originally wanted to compute embeddings.\n",
    "https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class NeighborSampler(RawNeighborSampler):\n",
    "    def sample(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "        row, col, _ = self.adj_t.coo()\n",
    "\n",
    "        pos_batch, neg_batch = custom_pos_sampling_with_POI(street_edges_weight_tensor, batch)\n",
    "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
    "        sampled = super().sample(batch)\n",
    "        return sampled\n",
    "\n",
    "\n",
    "train_loader = NeighborSampler(street_edges_index_tensor, sizes=NEIGHBOUR_SIZE, batch_size=256,\n",
    "                               shuffle=True, num_nodes=number_of_nodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SAGE(number_of_node_features, hidden_channels=HIDDEN_LAYER, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x, edge_index = street_nodes_features_tensor.to(device), street_edges_index_tensor.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "outputs": [],
   "source": [
    "def regression_train(embedding_df):\n",
    "    AKL_df = pd.read_csv(f\"{output_path}/datasets/property_data_with_street.csv\", encoding='latin1')\n",
    "    AKL_df = AKL_df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    akl_embedding_np = embedding_df.numpy()  #convert to Numpy array\n",
    "    akl_embedding_df = pd.DataFrame(akl_embedding_np)  #convert to a dataframe\n",
    "    embedding_size = akl_embedding_df.shape[1]\n",
    "    akl_embedding_df.columns = ['street_embedding_' + str(i) for i in range(embedding_size)]\n",
    "\n",
    "    akl_street_nodes_df = pd.read_csv(f\"{output_path}/datasets/akl_street_nodes.csv\")\n",
    "    akl_street_nodes_df = akl_street_nodes_df.rename(columns={\"source\": \"street_sources\", \"target\": \"street_targets\"})\n",
    "    AKL_df = find_embedding_for_property(AKL_df, akl_street_nodes_df, akl_embedding_df)\n",
    "    property_columns = ['CL_Suburb', 'CL_Sale_Tenure', 'CL_Sale_Date', 'CL_Land_Valuation_Capital_Value',\n",
    "                        'CL_Building_Floor_Area', 'CL_Building_Site_Cover',\n",
    "                        'CL_Land_Area', 'CL_Bldg_Const', 'CL_Bldg_Cond', 'CL_Roof_Const', 'CL_Roof_Cond',\n",
    "                        'CL_Category', 'CL_LUD_Age', 'CL_LUD_Land_Use_Description',\n",
    "                        'CL_MAS_No_Main_Roof_Garages', 'CL_Bedrooms', 'CL_Bathrooms'] + ['street_embedding_' + str(i)\n",
    "                                                                                         for i in range(embedding_size)]\n",
    "    X_columns = AKL_df[property_columns].values\n",
    "    #print(X_columns)\n",
    "    Y_column = AKL_df['Log_Sale_Price_Net'].values\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_columns, Y_column, test_size=0.2, random_state=1,\n",
    "                                                        shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1, shuffle=True)\n",
    "    hedonic_regression = LinearRegression()\n",
    "    hedonic_regression.fit(X_train, Y_train)\n",
    "\n",
    "    hedonic_regression_validation_result = hedonic_regression.predict(X_val)\n",
    "\n",
    "    validation_RMSE = round(mean_squared_error(Y_val, hedonic_regression_validation_result), 4)\n",
    "    validation_R2 = round(r2_score(Y_val, hedonic_regression_validation_result), 4)\n",
    "    return validation_RMSE, validation_R2\n",
    "\n",
    "\n",
    "def find_embedding_for_property(property_df, street_df, emb_df):\n",
    "    street_with_embedding = street_df.merge(emb_df, left_index=True, right_index=True)\n",
    "    output_df = property_df.merge(street_with_embedding, on=[\"street_sources\", \"street_targets\"])\n",
    "    return output_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36000/3810370810.py:83: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [495], line 36\u001B[0m\n\u001B[1;32m     34\u001B[0m historical_rmse, historical_r2, historical_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m20\u001B[39m):\n\u001B[0;32m---> 36\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "Cell \u001B[0;32mIn [495], line 6\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      5\u001B[0m i\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_size, n_id, adjs \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m      7\u001B[0m     i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# `adjs` holds a list of `(edge_index, e_id, size)` tuples.\u001B[39;00m\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 681\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    683\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    684\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    685\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    720\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 721\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    722\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    723\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [492], line 17\u001B[0m, in \u001B[0;36mNeighborSampler.sample\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     14\u001B[0m batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(batch)\n\u001B[1;32m     15\u001B[0m row, col, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madj_t\u001B[38;5;241m.\u001B[39mcoo()\n\u001B[0;32m---> 17\u001B[0m pos_batch, neg_batch \u001B[38;5;241m=\u001B[39m \u001B[43mcustom_pos_sampling_with_POI\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstreet_edges_weight_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([batch, pos_batch, neg_batch], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     19\u001B[0m sampled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msample(batch)\n",
      "Cell \u001B[0;32mIn [488], line 15\u001B[0m, in \u001B[0;36mcustom_pos_sampling_with_POI\u001B[0;34m(edge_weight, batch)\u001B[0m\n\u001B[1;32m     13\u001B[0m neighbour_id_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m neighbours_edge_index:\n\u001B[0;32m---> 15\u001B[0m     neighbour_id_list\u001B[38;5;241m.\u001B[39mappend(\u001B[43mselect_neighbour_node_id_from_edge\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcurrent_node_id\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     17\u001B[0m normalized_neighbour_poi_weights \u001B[38;5;241m=\u001B[39m calculate_normalized_poi_weights(neighbour_id_list,current_node_id)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# all neighbours don't have POI\u001B[39;00m\n",
      "Cell \u001B[0;32mIn [488], line 94\u001B[0m, in \u001B[0;36mselect_neighbour_node_id_from_edge\u001B[0;34m(next_edge_index, current_node_id)\u001B[0m\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cached_neighbour_node_id_dict[(current_node_id,next_edge_index)]\n\u001B[1;32m     93\u001B[0m next_edge_df \u001B[38;5;241m=\u001B[39m street_edges_df\u001B[38;5;241m.\u001B[39miloc[[next_edge_index]]\n\u001B[0;32m---> 94\u001B[0m next_edge \u001B[38;5;241m=\u001B[39m \u001B[43mnext_edge_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m next_edge[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m current_node_id:\n\u001B[1;32m     96\u001B[0m     neighbour_node_id \u001B[38;5;241m=\u001B[39m next_edge[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/pandas/core/frame.py:11726\u001B[0m, in \u001B[0;36mDataFrame.values\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m  11653\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m  11654\u001B[0m \u001B[38;5;124;03mReturn a Numpy representation of the DataFrame.\u001B[39;00m\n\u001B[1;32m  11655\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  11723\u001B[0m \u001B[38;5;124;03m       ['monkey', nan, None]], dtype=object)\u001B[39;00m\n\u001B[1;32m  11724\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m  11725\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consolidate_inplace()\n\u001B[0;32m> 11726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/pandas/core/internals/managers.py:1746\u001B[0m, in \u001B[0;36mBlockManager.as_array\u001B[0;34m(self, dtype, copy, na_value)\u001B[0m\n\u001B[1;32m   1744\u001B[0m             arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1746\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interleave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1747\u001B[0m     \u001B[38;5;66;03m# The underlying data was copied within _interleave\u001B[39;00m\n\u001B[1;32m   1748\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/pandas/core/internals/managers.py:1771\u001B[0m, in \u001B[0;36mBlockManager._interleave\u001B[0;34m(self, dtype, na_value)\u001B[0m\n\u001B[1;32m   1763\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1764\u001B[0m \u001B[38;5;124;03mReturn ndarray from blocks with specified item order\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;124;03mItems must be contained in the blocks\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dtype:\n\u001B[1;32m   1768\u001B[0m     \u001B[38;5;66;03m# Incompatible types in assignment (expression has type\u001B[39;00m\n\u001B[1;32m   1769\u001B[0m     \u001B[38;5;66;03m# \"Optional[Union[dtype[Any], ExtensionDtype]]\", variable has\u001B[39;00m\n\u001B[1;32m   1770\u001B[0m     \u001B[38;5;66;03m# type \"Optional[dtype[Any]]\")\u001B[39;00m\n\u001B[0;32m-> 1771\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m \u001B[43minterleaved_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[assignment]\u001B[39;49;00m\n\u001B[1;32m   1772\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mblk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mblk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocks\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m   1773\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;66;03m# TODO: https://github.com/pandas-dev/pandas/issues/22791\u001B[39;00m\n\u001B[1;32m   1776\u001B[0m \u001B[38;5;66;03m# Give EAs some input on what happens here. Sparse needs this.\u001B[39;00m\n\u001B[1;32m   1777\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, SparseDtype):\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/pandas/core/internals/base.py:226\u001B[0m, in \u001B[0;36minterleaved_dtype\u001B[0;34m(dtypes)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(dtypes):\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 226\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfind_common_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtypes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1628\u001B[0m, in \u001B[0;36mfind_common_type\u001B[0;34m(types)\u001B[0m\n\u001B[1;32m   1625\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1627\u001B[0m \u001B[38;5;66;03m# take lowest unit\u001B[39;00m\n\u001B[0;32m-> 1628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mall\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mis_datetime64_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatetime64[ns]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(is_timedelta64_dtype(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m types):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    i=0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        i+=1\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x[n_id].to(device), adjs)\n",
    "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "        print(i)\n",
    "    return total_loss / number_of_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_model_embedding():\n",
    "    model.eval()\n",
    "    embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "historical_rmse, historical_r2, historical_loss = np.inf, np.inf, np.inf\n",
    "for epoch in range(1, 20):\n",
    "    loss = train()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        temp_embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "        regression_rmse, regression_r2 = regression_train(temp_embedding)\n",
    "\n",
    "        if historical_rmse == np.inf:\n",
    "            historical_rmse = regression_rmse\n",
    "\n",
    "        if historical_r2 == np.inf:\n",
    "            historical_r2 = regression_r2\n",
    "\n",
    "        if historical_loss == np.inf:\n",
    "            historical_loss = loss\n",
    "\n",
    "        if loss > historical_loss:\n",
    "            if regression_rmse > historical_rmse:\n",
    "                print(\n",
    "                    f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on RMSE! Current RMSE is {regression_rmse}, previous RMSE is {historical_rmse} ')\n",
    "                break\n",
    "\n",
    "            if regression_r2 < historical_r2:\n",
    "                print(\n",
    "                    f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on r2! Current R2 is {regression_r2}, previous R2 is {historical_r2} ')\n",
    "                break\n",
    "        else:\n",
    "            historical_loss = loss\n",
    "            historical_rmse = regression_rmse\n",
    "            historical_r2 = regression_r2\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},Current RMSE is {regression_rmse}, Current R2 is {regression_r2} ')\n",
    "        output_embedding = temp_embedding\n",
    "\n",
    "# output_embedding = get_model_embedding()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(output_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_np = output_embedding.numpy()  #convert to Numpy array\n",
    "output_df = pd.DataFrame(output_np)  #convert to a dataframe\n",
    "current_GMT = time.gmtime()\n",
    "ts = calendar.timegm(current_GMT)\n",
    "output_df.to_csv(f\"./outputs/akl_embedding_{ts}.csv\", index=False)  #save to file\n",
    "print(f\"akl_embedding_{ts}.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
