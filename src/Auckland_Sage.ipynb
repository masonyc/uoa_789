{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score\n",
    "from typing import Optional, Tuple, Union\n",
    "import calendar\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler as RawNeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_path = \"/run/media/yunchen/lacie\"\n",
    "\n",
    "HIDDEN_LAYER = 16\n",
    "NUM_LAYERS = 3\n",
    "NEIGHBOUR_SIZE = [5, 5]\n",
    "DROP_OUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   restaurant  amenity  school  shop  healthcare  clothes\n",
      "0         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "1         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "2         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "3         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "4         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "458252\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = pd.read_csv(f\"{output_path}/outputs/akl_street_nodes.csv\")\n",
    "street_nodes_df = street_nodes_df[street_nodes_df.columns[4:]]\n",
    "\n",
    "street_nodes_df_copy = street_nodes_df.copy()\n",
    "street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\", \"x\", \"y\"], axis=1, inplace=True)\n",
    "print(street_nodes_df_copy.head())\n",
    "\n",
    "street_nodes_features_tensor = torch.tensor(street_nodes_df_copy.values.tolist())\n",
    "\n",
    "number_of_nodes = len(street_nodes_features_tensor)\n",
    "number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "print(street_nodes_features_tensor)\n",
    "print(number_of_nodes)\n",
    "print(number_of_node_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,      2,      0,  ..., 458250, 458250, 458251],\n",
      "        [     2,      0,      1,  ..., 458249, 458251, 458250]])\n",
      "tensor([ 96.8730,  96.8730, 100.7870,  ..., 328.7770, 493.5280, 493.5280])\n"
     ]
    }
   ],
   "source": [
    "street_edges_df = pd.read_csv(f\"{output_path}/outputs/akl_street_edges.csv\")\n",
    "source_street_index, targe_street_index, street_distance_weight = street_edges_df[\"source_street\"], street_edges_df[\n",
    "    \"target_street\"], street_edges_df[\"distance\"]\n",
    "street_edges_source_index_tensor = torch.tensor([source_street_index.values.tolist()])\n",
    "street_edges_target_index_tensor = torch.tensor([targe_street_index.values.tolist()])\n",
    "street_edges_index_tensor = torch.cat((street_edges_source_index_tensor, street_edges_target_index_tensor), 0)\n",
    "street_edges_weight_tensor = torch.tensor(street_distance_weight.values.tolist())\n",
    "\n",
    "print(street_edges_index_tensor)\n",
    "print(street_edges_weight_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def custom_pos_sampling(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    ") -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        total_distance = 0\n",
    "        current_node_id = start_node_id\n",
    "        # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # 选出edge对应的weight\n",
    "        neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "        norm_neighbour_weights = [i / sum(neighbour_weights.numpy()) for i in neighbour_weights.numpy()]\n",
    "        #根据概率随机选一个\n",
    "        #print(neighbours_edge_index,len(neighbour_weights))\n",
    "        if len(neighbour_weights) == 0:\n",
    "            current_node_seq.append(current_node_id)\n",
    "            pos_node_seq.append(current_node_seq)\n",
    "            #neg_node_seq.append(current_node_seq)\n",
    "            continue\n",
    "        neighbour_weights_index = np.random.choice(len(neighbour_weights), p=norm_neighbour_weights)\n",
    "\n",
    "        # print(\"current Node id \\n\", current_node_id)\n",
    "        #print(\"neighbour weights \\n\", neighbour_weights)\n",
    "        #print(\"neighbour weights index  \\n\", neighbour_weights_index)\n",
    "        #print(neighbour_weights.min(),neighbour_weights.argmin())\n",
    "\n",
    "        # 取最近的边\n",
    "        # TODO：加入别的策略，poi信息等\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "        #print(\"next edge \\n\", next_edge_df)\n",
    "        next_edge = next_edge_df.values[0]\n",
    "        total_distance += next_edge[2]\n",
    "        # next_edge[0] = source street\n",
    "        # next_edge[1] = target_street\n",
    "        # next_edge[2] = distance\n",
    "        if next_edge[0] != current_node_id:\n",
    "            current_node_id = next_edge[0]\n",
    "        else:\n",
    "            current_node_id = next_edge[1]\n",
    "        current_node_seq.append(current_node_id)\n",
    "        pos_node_seq.append(current_node_seq)\n",
    "    #if len(neg_node_seq) >0 :\n",
    "    #print(\"Isolated node: {number} {node_list}\".format(number = len(neg_node_seq),node_list = neg_node_seq))\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def custom_pos_sampling_with_POI(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    "):\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    poi_nodes = set()\n",
    "    no_poi_nodes = set()\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        current_node_id = current_node_seq[-1]\n",
    "        # 找距离？\n",
    "        # current_x,current_y = street_nodes_df.iloc[[start_node_id]][\"x\"],street_nodes_df.iloc[[start_node_id]][\"y\"]\n",
    "        # print(start_node_id)\n",
    "        # print(street_nodes_df.iloc[[start_node_id]])\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        neighbour_id_list = []\n",
    "        neighbour_id_index = []\n",
    "        for edge_index in neighbours_edge_index:\n",
    "            neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "            neighbour_edge = neighbour_edge_df.values[0]\n",
    "            if neighbour_edge[0] != current_node_id:\n",
    "                neighbour_id = neighbour_edge[0]\n",
    "            else:\n",
    "                neighbour_id = neighbour_edge[1]\n",
    "            neighbour_id_list.append([neighbour_id])\n",
    "            neighbour_id_index.append(0)\n",
    "\n",
    "        # steps 自动-1 比如想要3步的话 就传2\n",
    "        neighbour_id_list = find_neighbours(0, neighbour_id_list, neighbour_id_index)  #,current_x,current_y,500)\n",
    "        neighbour_id_weights = []\n",
    "        for neighbour_ids in neighbour_id_list:\n",
    "            poi_weight = 0\n",
    "            for neighbour_id in neighbour_ids:\n",
    "                neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "                                                        torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "                poi_weight += torch.sum(neighbour_features)\n",
    "            neighbour_id_weights.append(poi_weight)\n",
    "\n",
    "        # print(f\"neighbour_id_weights = {neighbour_id_weights}\")\n",
    "        neighbour_id_weights = np.array(neighbour_id_weights)\n",
    "        normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n",
    "\n",
    "        neighbour_weights_index = 0\n",
    "\n",
    "        if np.isnan(normalized_neighbour_weights).all():\n",
    "            no_poi_nodes.add(current_node_id)\n",
    "            if len(neighbours_edge_index) == 0:\n",
    "                current_node_seq.append(current_node_id)\n",
    "                pos_node_seq.append(current_node_seq)\n",
    "                if len(poi_nodes) == 0:\n",
    "                    init_neg_node = street_nodes_df.sample()\n",
    "                    # print(init_neg_node.index.values[0])\n",
    "                    neg_node_seq.append(init_neg_node.index.values[0])\n",
    "                else:\n",
    "                    neg_node_seq.append(np.random.choice(list(poi_nodes)))\n",
    "                continue\n",
    "            else:\n",
    "                neighbour_weights_index = np.random.choice(len(neighbours_edge_index))\n",
    "        else:\n",
    "            poi_nodes.add(current_node_id)\n",
    "            neighbour_weights_index = np.random.choice(len(normalized_neighbour_weights),\n",
    "                                                       p=normalized_neighbour_weights)\n",
    "\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "        next_edge = next_edge_df.values[0]\n",
    "        if next_edge[0] != current_node_id:\n",
    "            current_node_id = next_edge[0]\n",
    "        else:\n",
    "            current_node_id = next_edge[1]\n",
    "        current_node_seq.append(current_node_id)\n",
    "        pos_node_seq.append(current_node_seq)\n",
    "        if len(no_poi_nodes) == 0:\n",
    "            init_neg_node = street_nodes_df.sample()\n",
    "            # print(init_neg_node.index.values[0])\n",
    "            neg_node_seq.append(init_neg_node.index.values[0])\n",
    "        else:\n",
    "            neg_node_seq.append(np.random.choice(list(no_poi_nodes)))\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "        np.asarray(neg_node_seq, dtype=np.int32))\n",
    "\n",
    "\n",
    "# bfs like\n",
    "def find_neighbours(steps, neighbour_id_list, neighbour_id_index_list):  #,origin_x,origin_y,max_dist):\n",
    "    if steps <= 0:\n",
    "        return neighbour_id_list\n",
    "\n",
    "    current = neighbour_id_list\n",
    "    neighbour_id_index = []\n",
    "    for i, neighbour_list in enumerate(current):\n",
    "        short_neighbour_list = neighbour_list[neighbour_id_index_list[i]:]\n",
    "        neighbour_id_index.append(len(neighbour_list) + 1)\n",
    "        for neigh in short_neighbour_list:\n",
    "            neighbours_edge_index = (street_edges_index_tensor == neigh).nonzero(as_tuple=True)[1]\n",
    "\n",
    "            for edge_index in neighbours_edge_index:\n",
    "                neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "                neighbour_edge = neighbour_edge_df.values[0]\n",
    "                if neighbour_edge[0] != neigh:\n",
    "                    neighbour_id = neighbour_edge[0]\n",
    "                else:\n",
    "                    neighbour_id = neighbour_edge[1]\n",
    "\n",
    "                # 找距离？\n",
    "                # loc_x,loc_y = street_nodes_df.iloc[[neighbour_id]][\"x\"],street_nodes_df.iloc[[neighbour_id]][\"y\"]\n",
    "                # distance_to_origin = ox.distance.euclidean_dist_vec(loc_y,loc_x,\n",
    "                #                                                  origin_y,origin_x)\n",
    "                # if distance_to_origin > max_dist:\n",
    "                #     continue\n",
    "\n",
    "                neighbour_id_list[i].append(neighbour_id)\n",
    "    return find_neighbours(steps - 1, neighbour_id_list, neighbour_id_index)  #,origin_x,origin_y,max_dist)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def custom_neg_sampling(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    "        adj_size: int\n",
    ") -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "    neg_node_seq = []\n",
    "    for start_node_id in batch:\n",
    "        current_node_id = start_node_id\n",
    "        # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "        neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "        neighbour_weights_avg = np.average(neighbour_weights)\n",
    "\n",
    "        # 选出edge对应的weight\n",
    "        negative_neighbour_weights_min = 0\n",
    "        random_neg_index = torch.randint(0, 1, (1, 1), dtype=torch.long)\n",
    "        while negative_neighbour_weights_min < neighbour_weights_avg:\n",
    "            random_neg_index = torch.randint(0, adj_size, (1, 1), dtype=torch.long)\n",
    "            negative_neighbours_edge_index = (street_edges_index_tensor == random_neg_index).nonzero(as_tuple=True)[1]\n",
    "            negative_neighbour_weights = torch.index_select(edge_weight, 0, negative_neighbours_edge_index)\n",
    "            negative_neighbour_weights_min = min(negative_neighbour_weights)\n",
    "        neg_node_seq.append(random_neg_index.item())\n",
    "    return torch.from_numpy(np.array(neg_node_seq, dtype=np.compat.long))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RawNeighborSampler This module iteratively samples neighbors (at each layer) and constructs bipartite graphs that simulate the actual computation flow of GNNs.\n",
    "\n",
    "sizes: denotes how much neighbors we want to sample for each node in each layer.\n",
    "\n",
    "NeighborSampler holds the current :obj:batch_size, the IDs :obj:n_id of all nodes involved in the computation, and a list of bipartite graph objects via the tuple :obj:(edge_index, e_id, size), where :obj:edge_index represents the bipartite edges between source and target nodes, :obj:e_id denotes the IDs of original edges in the full graph, and :obj:size holds the shape of the bipartite graph.\n",
    "\n",
    "The actual computation graphs are then returned in reverse-mode, meaning that we pass messages from a larger set of nodes to a smaller one, until we reach the nodes for which we originally wanted to compute embeddings.\n",
    "https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class NeighborSampler(RawNeighborSampler):\n",
    "    def sample(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "        row, col, _ = self.adj_t.coo()\n",
    "\n",
    "        pos_batch, neg_batch = custom_pos_sampling_with_POI(street_edges_weight_tensor, batch)\n",
    "        #neg_batch = custom_neg_sampling(street_edges_weight_tensor, batch, self.adj_t.size(1))\n",
    "        # neg_batch = torch.randint(0, self.adj_t.size(1), (batch.numel(),), dtype=torch.long)\n",
    "        #print(\"Custom nodes seq,\", pos_batch)\n",
    "        #print(\"negative batch \\n \", neg_batch)\n",
    "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
    "        sampled = super().sample(batch)\n",
    "        return sampled\n",
    "\n",
    "\n",
    "train_loader = NeighborSampler(street_edges_index_tensor, sizes=NEIGHBOUR_SIZE, batch_size=256,\n",
    "                               shuffle=True, num_nodes=number_of_nodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SAGE(number_of_node_features, hidden_channels=HIDDEN_LAYER, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x, edge_index = street_nodes_features_tensor.to(device), street_edges_index_tensor.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def regression_train(embedding_df):\n",
    "    AKL_df = pd.read_csv(f\"{output_path}/outputs/property_data_with_street.csv\", encoding='latin1')\n",
    "    AKL_df = AKL_df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    akl_embedding_np = embedding_df.numpy()  #convert to Numpy array\n",
    "    akl_embedding_df = pd.DataFrame(akl_embedding_np)  #convert to a dataframe\n",
    "    embedding_size = akl_embedding_df.shape[1]\n",
    "    akl_embedding_df.columns = ['street_embedding_' + str(i) for i in range(embedding_size)]\n",
    "\n",
    "    akl_street_nodes_df = pd.read_csv(f\"{output_path}/outputs/akl_street_nodes.csv\")\n",
    "    akl_street_nodes_df = akl_street_nodes_df.rename(columns={\"source\": \"street_sources\", \"target\": \"street_targets\"})\n",
    "    AKL_df = find_embedding_for_property(AKL_df, akl_street_nodes_df, akl_embedding_df)\n",
    "    property_columns = ['CL_Suburb', 'CL_Sale_Tenure', 'CL_Sale_Date', 'CL_Land_Valuation_Capital_Value',\n",
    "                        'CL_Building_Floor_Area', 'CL_Building_Site_Cover',\n",
    "                        'CL_Land_Area', 'CL_Bldg_Const', 'CL_Bldg_Cond', 'CL_Roof_Const', 'CL_Roof_Cond',\n",
    "                        'CL_Category', 'CL_LUD_Age', 'CL_LUD_Land_Use_Description',\n",
    "                        'CL_MAS_No_Main_Roof_Garages', 'CL_Bedrooms', 'CL_Bathrooms'] + ['street_embedding_' + str(i)\n",
    "                                                                                         for i in range(embedding_size)]\n",
    "    X_columns = AKL_df[property_columns].values\n",
    "    #print(X_columns)\n",
    "    Y_column = AKL_df['Log_Sale_Price_Net'].values\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_columns, Y_column, test_size=0.2, random_state=1,\n",
    "                                                        shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1, shuffle=True)\n",
    "    hedonic_regression = LinearRegression()\n",
    "    hedonic_regression.fit(X_train, Y_train)\n",
    "\n",
    "    hedonic_regression_validation_result = hedonic_regression.predict(X_val)\n",
    "\n",
    "    validation_RMSE = round(mean_squared_error(Y_val, hedonic_regression_validation_result), 4)\n",
    "    validation_R2 = round(r2_score(Y_val, hedonic_regression_validation_result), 4)\n",
    "    return validation_RMSE, validation_R2\n",
    "\n",
    "\n",
    "def find_embedding_for_property(property_df, street_df, emb_df):\n",
    "    street_with_embedding = street_df.merge(emb_df, left_index=True, right_index=True)\n",
    "    output_df = property_df.merge(street_with_embedding, on=[\"street_sources\", \"street_targets\"])\n",
    "    return output_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 001, Loss: 1.3406,Current RMSE is 0.0594, Current R2 is 0.7983 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 002, Loss: 1.3337,Current RMSE is 0.06, Current R2 is 0.7965 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 003, Loss: 1.3273,Current RMSE is 0.0598, Current R2 is 0.7971 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 004, Loss: 1.3262,Current RMSE is 0.0598, Current R2 is 0.7971 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 005, Loss: 1.3229,Current RMSE is 0.0597, Current R2 is 0.7975 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 006, Loss: 1.3228,Current RMSE is 0.0597, Current R2 is 0.7975 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 007, Loss: 1.3223,Current RMSE is 0.0599, Current R2 is 0.7967 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 008, Loss: 1.3205,Current RMSE is 0.0598, Current R2 is 0.7971 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 009, Loss: 1.3196,Current RMSE is 0.0601, Current R2 is 0.7961 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 010, Loss: 1.3191,Current RMSE is 0.0599, Current R2 is 0.7967 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 011, Loss: 1.3181,Current RMSE is 0.0598, Current R2 is 0.7972 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 012, Loss: 1.3165,Current RMSE is 0.0606, Current R2 is 0.7944 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 013, Loss: 1.3184,Current RMSE is 0.0589, Current R2 is 0.8001 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 014, Loss: 1.3170,Current RMSE is 0.0599, Current R2 is 0.7967 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 015, Loss: 1.3160,Current RMSE is 0.0597, Current R2 is 0.7973 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 016, Loss: 1.3150,Current RMSE is 0.0596, Current R2 is 0.7979 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2799/2281864792.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n",
      "Epoch: 017, Loss: 1.3166, Stopped on RMSE! Current RMSE is 0.0603, previous RMSE is 0.0596 \n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        i += 1\n",
    "        #print(i)\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x[n_id], adjs)\n",
    "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "    print(i)\n",
    "    return total_loss / number_of_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_model_embedding():\n",
    "    model.eval()\n",
    "    embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "historical_rmse, historical_r2, historical_loss = np.inf, np.inf, np.inf\n",
    "for epoch in range(1, 20):\n",
    "    loss = train()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        temp_embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "        regression_rmse, regression_r2 = regression_train(temp_embedding)\n",
    "\n",
    "        if historical_rmse == np.inf:\n",
    "            historical_rmse = regression_rmse\n",
    "\n",
    "        if historical_r2 == np.inf:\n",
    "            historical_r2 = regression_r2\n",
    "\n",
    "        if historical_loss == np.inf:\n",
    "            historical_loss = loss\n",
    "\n",
    "        if loss > historical_loss:\n",
    "            if regression_rmse > historical_rmse:\n",
    "                print(\n",
    "                    f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on RMSE! Current RMSE is {regression_rmse}, previous RMSE is {historical_rmse} ')\n",
    "                break\n",
    "\n",
    "            if regression_r2 < historical_r2:\n",
    "                print(\n",
    "                    f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on r2! Current R2 is {regression_r2}, previous R2 is {historical_r2} ')\n",
    "                break\n",
    "        else:\n",
    "            historical_loss = loss\n",
    "            historical_rmse = regression_rmse\n",
    "            historical_r2 = regression_r2\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},Current RMSE is {regression_rmse}, Current R2 is {regression_r2} ')\n",
    "        output_embedding = temp_embedding\n",
    "\n",
    "# output_embedding = get_model_embedding()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1491, -0.0628,  0.0477,  ...,  0.0284,  0.0374,  0.2132],\n",
      "        [-0.1491, -0.0628,  0.0477,  ...,  0.0284,  0.0374,  0.2132],\n",
      "        [-0.1491, -0.0628,  0.0477,  ...,  0.0284,  0.0374,  0.2132],\n",
      "        ...,\n",
      "        [-0.1491, -0.0628,  0.0477,  ...,  0.0284,  0.0374,  0.2132],\n",
      "        [-0.1491, -0.0628,  0.0477,  ...,  0.0284,  0.0374,  0.2132],\n",
      "        [-0.1491, -0.0628,  0.0477,  ...,  0.0284,  0.0374,  0.2132]])\n"
     ]
    }
   ],
   "source": [
    "print(output_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akl_embedding_1667781757.csv\n"
     ]
    }
   ],
   "source": [
    "output_np = output_embedding.numpy()  #convert to Numpy array\n",
    "output_df = pd.DataFrame(output_np)  #convert to a dataframe\n",
    "current_GMT = time.gmtime()\n",
    "ts = calendar.timegm(current_GMT)\n",
    "output_df.to_csv(f\"./outputs/akl_embedding_{ts}.csv\", index=False)  #save to file\n",
    "print(f\"akl_embedding_{ts}.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
