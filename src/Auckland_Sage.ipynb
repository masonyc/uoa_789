{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score\n",
    "from typing import Optional, Tuple, Union\n",
    "import calendar\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler as RawNeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_path = \"/run/media/yunchen/lacie\"\n",
    "# output_path = \".\"\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "HIDDEN_LAYER =16\n",
    "NUM_LAYERS = 5\n",
    "NEIGHBOUR_SIZE = [5, 5]\n",
    "DROP_OUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,      2,      0,  ..., 463095, 463095, 463096],\n",
      "        [     2,      0,      1,  ..., 463094, 463096, 463095]])\n",
      "tensor([ 96.8730,  96.8730, 100.7870,  ..., 328.7770, 493.5280, 493.5280])\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = pd.read_csv(f\"{output_path}/datasets/akl_street_nodes.csv\")\n",
    "street_edges_df = pd.read_csv(f\"{output_path}/datasets/akl_street_edges.csv\")\n",
    "source_street_index, targe_street_index, street_distance_weight = street_edges_df[\"source_street\"], street_edges_df[\n",
    "    \"target_street\"], street_edges_df[\"distance\"]\n",
    "\n",
    "# isolated_list = []\n",
    "# non_isolated_source = set(source_street_index.values.tolist())\n",
    "# non_isolated_target = set(targe_street_index.values.tolist())\n",
    "# for isolated_i,_ in street_nodes_df.iterrows():\n",
    "#     if isolated_i not in non_isolated_source and isolated_i  not in non_isolated_target:\n",
    "#         isolated_list.append(isolated_i)\n",
    "# street_nodes_df.drop(axis=0,index = isolated_list,inplace=True)\n",
    "\n",
    "street_edges_source_index_tensor = torch.tensor([source_street_index.values.tolist()])\n",
    "street_edges_target_index_tensor = torch.tensor([targe_street_index.values.tolist()])\n",
    "street_edges_index_tensor = torch.cat((street_edges_source_index_tensor, street_edges_target_index_tensor), 0)\n",
    "street_edges_weight_tensor = torch.tensor(street_distance_weight.values.tolist())\n",
    "\n",
    "print(street_edges_index_tensor)\n",
    "print(street_edges_weight_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         street_length  restaurant  Average_POI_Distance  school  amenity  \\\n",
      "0              32.641         0.0                32.641     0.0      0.0   \n",
      "1              68.146         0.0                68.146     0.0      0.0   \n",
      "2              64.232         0.0                64.232     0.0      0.0   \n",
      "3              50.560         0.0                50.560     0.0      0.0   \n",
      "4              20.232         0.0                20.232     0.0      0.0   \n",
      "...               ...         ...                   ...     ...      ...   \n",
      "463092         25.652         0.0                25.652     0.0      0.0   \n",
      "463093        166.770         0.0               166.770     0.0      0.0   \n",
      "463094        143.628         0.0               143.628     0.0      0.0   \n",
      "463095        185.149         0.0               185.149     0.0      0.0   \n",
      "463096        308.379         0.0               308.379     0.0      0.0   \n",
      "\n",
      "        shop  healthcare  clothes  \n",
      "0        0.0         0.0      0.0  \n",
      "1        0.0         0.0      0.0  \n",
      "2        0.0         0.0      0.0  \n",
      "3        0.0         0.0      0.0  \n",
      "4        0.0         0.0      0.0  \n",
      "...      ...         ...      ...  \n",
      "463092   0.0         0.0      0.0  \n",
      "463093   0.0         0.0      0.0  \n",
      "463094   0.0         0.0      0.0  \n",
      "463095   0.0         0.0      0.0  \n",
      "463096   0.0         0.0      0.0  \n",
      "\n",
      "[463097 rows x 8 columns]>\n",
      "Index(['restaurant', 'school', 'amenity', 'shop', 'healthcare', 'clothes'], dtype='object')\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "463097\n",
      "6\n",
      "7051\n",
      "456046\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = street_nodes_df[street_nodes_df.columns[4:]]\n",
    "\n",
    "street_nodes_df_copy = street_nodes_df.copy()\n",
    "print(street_nodes_df_copy.head)\n",
    "street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\",\"x\",\"y\"], axis=1, inplace=True)\n",
    "# street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\"], axis=1, inplace=True)\n",
    "print(street_nodes_df_copy.columns)\n",
    "\n",
    "street_nodes_features_tensor = torch.tensor(street_nodes_df_copy.values.tolist())\n",
    "\n",
    "number_of_nodes = len(street_nodes_features_tensor)\n",
    "number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "print(street_nodes_features_tensor)\n",
    "print(number_of_nodes)\n",
    "print(number_of_node_features)\n",
    "\n",
    "street_nodes_df_copy_2 = street_nodes_df.copy()\n",
    "street_nodes_df_copy_2[\"poi_count\"] = street_nodes_df_copy_2.apply(lambda row:row.amenity+row.school+row.shop+row.healthcare+row.clothes,axis=1)\n",
    "positive_nodes_with_poi_df = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]>0]\n",
    "negative_nodes_without_poi_df = street_nodes_df_copy_2[street_nodes_df_copy_2[\"poi_count\"]<=0]\n",
    "print(len(positive_nodes_with_poi_df))\n",
    "print(len(negative_nodes_without_poi_df))\n",
    "# positive_nodes_index = torch.tensor(positive_nodes_with_poi_df.index)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def custom_pos_sampling_with_POI(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    "):\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        current_node_id = current_node_seq[-1]\n",
    "\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "        len_neighbours_edge_index = len(neighbours_edge_index)\n",
    "        neighbour_id_list = []\n",
    "        for i in neighbours_edge_index:\n",
    "            neighbour_id_list.append(select_neighbour_node_id_from_edge(i,current_node_id))\n",
    "\n",
    "        normalized_neighbour_poi_weights = calculate_normalized_poi_weights(neighbour_id_list)\n",
    "\n",
    "        # all neighbours don't have POI\n",
    "        if np.isnan(normalized_neighbour_poi_weights).all():\n",
    "            global_node_with_poi = positive_nodes_with_poi_df.sample(n=1,replace=True).index.values[0]\n",
    "            # Also no neighbour\n",
    "            if len_neighbours_edge_index == 0:\n",
    "                pos_node_seq.append([current_node_id,current_node_id])\n",
    "                neg_node_seq.append(global_node_with_poi)\n",
    "                # neg_node_seq.append(current_node_id)\n",
    "                # pos_node_seq.append([global_node_with_poi,global_node_with_poi])\n",
    "                continue\n",
    "            # No neighbour has POIs but do have neighbours\n",
    "            else:\n",
    "                # Pick a neighbour randomly using distance distribution to give future poi opportunity\n",
    "                neighbour_distance_weights = torch.index_select(edge_weight, 0, neighbours_edge_index).numpy()\n",
    "                norm_neighbour_distance_weights = calculate_normalized_distance_weights(neighbour_distance_weights)\n",
    "                neighbour_weights_index = np.random.choice(len(norm_neighbour_distance_weights),p=norm_neighbour_distance_weights)\n",
    "\n",
    "                pos_node_seq.append([current_node_id,neighbour_weights_index])\n",
    "                neg_node_seq.append(global_node_with_poi)\n",
    "                continue\n",
    "        # normal case pick with poi distribution\n",
    "        else:\n",
    "            neighbour_weights_index = np.random.choice(len(normalized_neighbour_poi_weights),\n",
    "                                                       p=normalized_neighbour_poi_weights)\n",
    "\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_node_id = select_neighbour_node_id_from_edge(next_edge_index,current_node_id)\n",
    "\n",
    "        global_node_without_poi = negative_nodes_without_poi_df.sample(replace=True).index.values[0]\n",
    "\n",
    "        current_node_seq.append(next_node_id)\n",
    "        pos_node_seq.append(current_node_seq)\n",
    "        neg_node_seq.append(global_node_without_poi)\n",
    "\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "        np.asarray(neg_node_seq, dtype=np.int32))\n",
    "\n",
    "def calculate_normalized_distance_weights(neighbour_distance_weights):\n",
    "    neighbour_distance_weights_sum = sum(neighbour_distance_weights)\n",
    "    reverted_norm_neighbour_distance_weights = [1-(i / neighbour_distance_weights_sum) for i in neighbour_distance_weights]\n",
    "\n",
    "    reverted_norm_neighbour_distance_weights_sum = sum(reverted_norm_neighbour_distance_weights)\n",
    "    norm_neighbour_distance_weights = [(i / reverted_norm_neighbour_distance_weights_sum) for i in reverted_norm_neighbour_distance_weights]\n",
    "    return norm_neighbour_distance_weights\n",
    "\n",
    "def calculate_normalized_poi_weights(neighbour_id_list):\n",
    "    neighbour_id_weights = []\n",
    "    for neighbour_id in neighbour_id_list:\n",
    "        poi_weight = 0\n",
    "        neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "                                                torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "        poi_weight += torch.sum(neighbour_features)\n",
    "        neighbour_id_weights.append(poi_weight)\n",
    "\n",
    "    neighbour_poi_weights = np.array(neighbour_id_weights)\n",
    "    neighbour_poi_weights_sum=sum(neighbour_poi_weights)\n",
    "    normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
    "    return normalized_neighbour_poi_weights\n",
    "\n",
    "def select_neighbour_node_id_from_edge(next_edge_index,current_node_id):\n",
    "    next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "    next_edge = next_edge_df.values[0]\n",
    "    if next_edge[0] != current_node_id:\n",
    "        neighbour_node_id = next_edge[0]\n",
    "    else:\n",
    "        neighbour_node_id = next_edge[1]\n",
    "\n",
    "    return neighbour_node_id\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# def custom_pos_sampling(\n",
    "#         edge_weight: tensor,\n",
    "#         batch: tensor,\n",
    "# ) -> union[tensor, tuple[tensor, tensor]]:\n",
    "#     pos_node_seq = []\n",
    "#     neg_node_seq = []\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_seq = [start_node_id.item()]\n",
    "#         total_distance = 0\n",
    "#         current_node_id = start_node_id\n",
    "#         # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=true)[1]\n",
    "#\n",
    "#         # 选出edge对应的weight\n",
    "#         neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "#         norm_neighbour_weights = [i / sum(neighbour_weights.numpy()) for i in neighbour_weights.numpy()]\n",
    "#         #根据概率随机选一个\n",
    "#         #print(neighbours_edge_index,len(neighbour_weights))\n",
    "#         if len(neighbour_weights) == 0:\n",
    "#             current_node_seq.append(current_node_id)\n",
    "#             pos_node_seq.append(current_node_seq)\n",
    "#             #neg_node_seq.append(current_node_seq)\n",
    "#             continue\n",
    "#         neighbour_weights_index = np.random.choice(len(neighbour_weights), p=norm_neighbour_weights)\n",
    "#\n",
    "#         # print(\"current node id \\n\", current_node_id)\n",
    "#         #print(\"neighbour weights \\n\", neighbour_weights)\n",
    "#         #print(\"neighbour weights index  \\n\", neighbour_weights_index)\n",
    "#         #print(neighbour_weights.min(),neighbour_weights.argmin())\n",
    "#\n",
    "#         # 取最近的边\n",
    "#         # todo：加入别的策略，poi信息等\n",
    "#         next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "#         next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "#         #print(\"next edge \\n\", next_edge_df)\n",
    "#         next_edge = next_edge_df.values[0]\n",
    "#         total_distance += next_edge[2]\n",
    "#         # next_edge[0] = source street\n",
    "#         # next_edge[1] = target_street\n",
    "#         # next_edge[2] = distance\n",
    "#         if next_edge[0] != current_node_id:\n",
    "#             current_node_id = next_edge[0]\n",
    "#         else:\n",
    "#             current_node_id = next_edge[1]\n",
    "#         current_node_seq.append(current_node_id)\n",
    "#         pos_node_seq.append(current_node_seq)\n",
    "#     #if len(neg_node_seq) >0 :\n",
    "#     #print(\"isolated node: {number} {node_list}\".format(number = len(neg_node_seq),node_list = neg_node_seq))\n",
    "#     return torch.from_numpynumpy(np.asarray(pos_node_seq, dtype=np.int32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# def custom_pos_sampling_with_POI(\n",
    "#         edge_weight: Tensor,\n",
    "#         batch: Tensor,\n",
    "# ):\n",
    "#     pos_node_seq = []\n",
    "#     neg_node_seq = []\n",
    "#     poi_nodes = set()\n",
    "#     no_poi_nodes = set()\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_seq = [start_node_id.item()]\n",
    "#         current_node_id = current_node_seq[-1]\n",
    "#         # 找距离？\n",
    "#         # current_x,current_y = street_nodes_df.iloc[[start_node_id]][\"x\"],street_nodes_df.iloc[[start_node_id]][\"y\"]\n",
    "#         # print(start_node_id)\n",
    "#         # print(street_nodes_df.iloc[[start_node_id]])\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "#\n",
    "#         neighbour_id_list = []\n",
    "#         # neighbour_id_index = []\n",
    "#         for edge_index in neighbours_edge_index:\n",
    "#             neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "#             neighbour_edge = neighbour_edge_df.values[0]\n",
    "#             if neighbour_edge[0] != current_node_id:\n",
    "#                 neighbour_id = neighbour_edge[0]\n",
    "#             else:\n",
    "#                 neighbour_id = neighbour_edge[1]\n",
    "#             neighbour_id_list.append([neighbour_id])\n",
    "#             # neighbour_id_index.append(0)\n",
    "#         #\n",
    "#         # # steps 自动-1 比如想要3步的话 就传2\n",
    "#         # neighbour_id_list = find_neighbours(0, neighbour_id_list, neighbour_id_index)  #,current_x,current_y,500)\n",
    "#         neighbour_id_weights = []\n",
    "#         # start at 36\n",
    "#         for neighbour_ids in neighbour_id_list:\n",
    "#             poi_weight = 0\n",
    "#             for neighbour_id in neighbour_ids:\n",
    "#                 neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "#                                                         torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "#                 poi_weight += torch.sum(neighbour_features)\n",
    "#             neighbour_id_weights.append(poi_weight)\n",
    "#\n",
    "#         # print(f\"neighbour_id_weights = {neighbour_id_weights}\")\n",
    "#         neighbour_id_weights = np.array(neighbour_id_weights)\n",
    "#         normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n",
    "#\n",
    "#         neighbour_weights_index = 0\n",
    "#\n",
    "#         if np.isnan(normalized_neighbour_weights).all():\n",
    "#             no_poi_nodes.add(current_node_id)\n",
    "#             if len(neighbours_edge_index) == 0:\n",
    "#                 current_node_seq.append(current_node_id)\n",
    "#                 pos_node_seq.append(current_node_seq)\n",
    "#                 if len(poi_nodes) == 0:\n",
    "#                     init_neg_node = street_nodes_df.sample()\n",
    "#                     # print(init_neg_node.index.values[0])\n",
    "#                     neg_node_seq.append(init_neg_node.index.values[0])\n",
    "#                 else:\n",
    "#                     neg_node_seq.append(np.random.choice(list(poi_nodes)))\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 neighbour_weights_index = np.random.choice(len(neighbours_edge_index))\n",
    "#         else:\n",
    "#             poi_nodes.add(current_node_id)\n",
    "#             neighbour_weights_index = np.random.choice(len(normalized_neighbour_weights),\n",
    "#                                                        p=normalized_neighbour_weights)\n",
    "#\n",
    "#         next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "#         next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "#         next_edge = next_edge_df.values[0]\n",
    "#         if next_edge[0] != current_node_id:\n",
    "#             current_node_id = next_edge[0]\n",
    "#         else:\n",
    "#             current_node_id = next_edge[1]\n",
    "#         current_node_seq.append(current_node_id)\n",
    "#         pos_node_seq.append(current_node_seq)\n",
    "#         if len(no_poi_nodes) == 0:\n",
    "#             init_neg_node = street_nodes_df.sample()\n",
    "#             # print(init_neg_node.index.values[0])\n",
    "#             neg_node_seq.append(init_neg_node.index.values[0])\n",
    "#         else:\n",
    "#             neg_node_seq.append(np.random.choice(list(no_poi_nodes)))\n",
    "#     return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1], torch.from_numpy(\n",
    "#         np.asarray(neg_node_seq, dtype=np.int32))\n",
    "#\n",
    "#\n",
    "# # bfs like\n",
    "# def find_neighbours(steps, neighbour_id_list, neighbour_id_index_list):  #,origin_x,origin_y,max_dist):\n",
    "#     if steps <= 0:\n",
    "#         return neighbour_id_list\n",
    "#\n",
    "#     current = neighbour_id_list\n",
    "#     neighbour_id_index = []\n",
    "#     for i, neighbour_list in enumerate(current):\n",
    "#         short_neighbour_list = neighbour_list[neighbour_id_index_list[i]:]\n",
    "#         neighbour_id_index.append(len(neighbour_list) + 1)\n",
    "#         for neigh in short_neighbour_list:\n",
    "#             neighbours_edge_index = (street_edges_index_tensor == neigh).nonzero(as_tuple=True)[1]\n",
    "#\n",
    "#             for edge_index in neighbours_edge_index:\n",
    "#                 neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "#                 neighbour_edge = neighbour_edge_df.values[0]\n",
    "#                 if neighbour_edge[0] != neigh:\n",
    "#                     neighbour_id = neighbour_edge[0]\n",
    "#                 else:\n",
    "#                     neighbour_id = neighbour_edge[1]\n",
    "#\n",
    "#                 # 找距离？\n",
    "#                 # loc_x,loc_y = street_nodes_df.iloc[[neighbour_id]][\"x\"],street_nodes_df.iloc[[neighbour_id]][\"y\"]\n",
    "#                 # distance_to_origin = ox.distance.euclidean_dist_vec(loc_y,loc_x,\n",
    "#                 #                                                  origin_y,origin_x)\n",
    "#                 # if distance_to_origin > max_dist:\n",
    "#                 #     continue\n",
    "#\n",
    "#                 neighbour_id_list[i].append(neighbour_id)\n",
    "#     return find_neighbours(steps - 1, neighbour_id_list, neighbour_id_index)  #,origin_x,origin_y,max_dist)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# def custom_neg_sampling(\n",
    "#         edge_weight: Tensor,\n",
    "#         batch: Tensor,\n",
    "#         adj_size: int\n",
    "# ) -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "#     neg_node_seq = []\n",
    "#     for start_node_id in batch:\n",
    "#         current_node_id = start_node_id\n",
    "#         # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "#         neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "#         neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "#         neighbour_weights_avg = np.average(neighbour_weights)\n",
    "#\n",
    "#         # 选出edge对应的weight\n",
    "#         negative_neighbour_weights_min = 0\n",
    "#         random_neg_index = torch.randint(0, 1, (1, 1), dtype=torch.long)\n",
    "#         while negative_neighbour_weights_min < neighbour_weights_avg:\n",
    "#             random_neg_index = torch.randint(0, adj_size, (1, 1), dtype=torch.long)\n",
    "#             negative_neighbours_edge_index = (street_edges_index_tensor == random_neg_index).nonzero(as_tuple=True)[1]\n",
    "#             negative_neighbour_weights = torch.index_select(edge_weight, 0, negative_neighbours_edge_index)\n",
    "#             negative_neighbour_weights_min = min(negative_neighbour_weights)\n",
    "#         neg_node_seq.append(random_neg_index.item())\n",
    "#     return torch.from_numpy(np.array(neg_node_seq, dtype=np.compat.long))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RawNeighborSampler This module iteratively samples neighbors (at each layer) and constructs bipartite graphs that simulate the actual computation flow of GNNs.\n",
    "\n",
    "format-selected)\n",
    "NeighborSampler holds the current :obj:batch_size, the IDs :obj:n_id of all nodes involved in the computation, and a list of bipartite graph objects via the tuple :obj:(edge_index, e_id, size), where :obj:edge_index represents the bipartite edges between source and target nodes, :obj:e_id denotes the IDs of original edges in the full graph, and :obj:size holds the shape of the bipartite graph.\n",
    "\n",
    "The actual computation graphs are then returned in reverse-mode, meaning that we pass messages from a larger set of nodes to a smaller one, until we reach the nodes for which we originally wanted to compute embeddings.\n",
    "https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "https://towardsdatascience.com/pytorch-geometric-graph-embedding-da71d614c3a\n",
    "https://gist.github.com/anuradhawick/904e7f2d2101f4b76516d04046007426\n",
    "https://zhuanlan.zhihu.com/p/387262710\n",
    "\"\"\"\n",
    "\n",
    "class NeighborSampler(RawNeighborSampler):\n",
    "    def sample(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "\n",
    "        pos_batch, neg_batch = custom_pos_sampling_with_POI(street_edges_weight_tensor, batch)\n",
    "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
    "        return super(NeighborSampler,self).sample(batch)\n",
    "\n",
    "train_loader = NeighborSampler(street_edges_index_tensor, sizes=NEIGHBOUR_SIZE, batch_size=1024, num_nodes=number_of_nodes,\n",
    "                                num_workers=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            # print(f\"x_target {x_target}\")\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=DROP_OUT, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SAGE(number_of_node_features, hidden_channels=HIDDEN_LAYER, num_layers=NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x, edge_index = street_nodes_features_tensor.to(device), street_edges_index_tensor.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def regression_train(embedding_df):\n",
    "    AKL_df = pd.read_csv(f\"{output_path}/datasets/property_data_with_street.csv\", encoding='latin1')\n",
    "    AKL_df = AKL_df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    akl_embedding_np = embedding_df.numpy()  #convert to Numpy array\n",
    "    akl_embedding_df = pd.DataFrame(akl_embedding_np)  #convert to a dataframe\n",
    "    embedding_size = akl_embedding_df.shape[1]\n",
    "    akl_embedding_df.columns = ['street_embedding_' + str(i) for i in range(embedding_size)]\n",
    "\n",
    "    akl_street_nodes_df = pd.read_csv(f\"{output_path}/datasets/akl_street_nodes.csv\")\n",
    "    akl_street_nodes_df = akl_street_nodes_df.rename(columns={\"source\": \"street_sources\", \"target\": \"street_targets\"})\n",
    "    AKL_df = find_embedding_for_property(AKL_df, akl_street_nodes_df, akl_embedding_df)\n",
    "    property_columns = ['CL_Suburb', 'CL_Sale_Tenure', 'CL_Sale_Date', 'CL_Land_Valuation_Capital_Value',\n",
    "                        'CL_Building_Floor_Area', 'CL_Building_Site_Cover',\n",
    "                        'CL_Land_Area', 'CL_Bldg_Const', 'CL_Bldg_Cond', 'CL_Roof_Const', 'CL_Roof_Cond',\n",
    "                        'CL_Category', 'CL_LUD_Age', 'CL_LUD_Land_Use_Description',\n",
    "                        'CL_MAS_No_Main_Roof_Garages', 'CL_Bedrooms', 'CL_Bathrooms'] + ['street_embedding_' + str(i)\n",
    "                                                                                         for i in range(embedding_size)]\n",
    "    X_columns = AKL_df[property_columns].values\n",
    "    #print(X_columns)\n",
    "    Y_column = AKL_df['Log_Sale_Price_Net'].values\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_columns, Y_column, test_size=0.2, random_state=1,\n",
    "                                                        shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1, shuffle=True)\n",
    "    hedonic_regression = LinearRegression()\n",
    "    hedonic_regression.fit(X_train, Y_train)\n",
    "\n",
    "    hedonic_regression_validation_result = hedonic_regression.predict(X_val)\n",
    "\n",
    "    validation_RMSE = round(mean_squared_error(Y_val, hedonic_regression_validation_result), 4)\n",
    "    validation_R2 = round(r2_score(Y_val, hedonic_regression_validation_result), 4)\n",
    "    return validation_RMSE, validation_R2\n",
    "\n",
    "\n",
    "def find_embedding_for_property(property_df, street_df, emb_df):\n",
    "    street_with_embedding = street_df.merge(emb_df, left_index=True, right_index=True)\n",
    "    output_df = property_df.merge(street_with_embedding, on=[\"street_sources\", \"street_targets\"])\n",
    "    return output_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7978,Current RMSE is 0.0598, Current R2 is 0.7772 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n",
      "/tmp/ipykernel_65923/4084092453.py:75: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  normalized_neighbour_poi_weights = [i / neighbour_poi_weights_sum for i in neighbour_poi_weights]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [25], line 36\u001B[0m\n\u001B[1;32m     34\u001B[0m historical_rmse, historical_r2, historical_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m20\u001B[39m):\n\u001B[0;32m---> 36\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "Cell \u001B[0;32mIn [25], line 6\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# i=0\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_size, n_id, adjs \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# i+=1\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# `adjs` holds a list of `(edge_index, e_id, size)` tuples.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     adjs \u001B[38;5;241m=\u001B[39m [adj\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m adj \u001B[38;5;129;01min\u001B[39;00m adjs]\n\u001B[1;32m     10\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 681\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    683\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    684\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    685\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1356\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1359\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1360\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1361\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1362\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1325\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1321\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1322\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1323\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1324\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1325\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1326\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1327\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1163\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[1;32m   1151\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[1;32m   1152\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1160\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[1;32m   1161\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[1;32m   1162\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1163\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1164\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[1;32m   1165\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1166\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[1;32m   1167\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[1;32m   1168\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/multiprocessing/queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m    112\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/multiprocessing/connection.py:262\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/multiprocessing/connection.py:429\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 429\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/multiprocessing/connection.py:936\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    933\u001B[0m     deadline \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic() \u001B[38;5;241m+\u001B[39m timeout\n\u001B[1;32m    935\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 936\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    937\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n\u001B[1;32m    938\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [key\u001B[38;5;241m.\u001B[39mfileobj \u001B[38;5;28;01mfor\u001B[39;00m (key, events) \u001B[38;5;129;01min\u001B[39;00m ready]\n",
      "File \u001B[0;32m/run/media/yunchen/lacie/projects/uoa_789_conda/lib/python3.9/selectors.py:416\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    414\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 416\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    # i=0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # i+=1\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x[n_id].to(device), adjs)\n",
    "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "        # print(i)\n",
    "    return total_loss / number_of_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_model_embedding():\n",
    "    model.eval()\n",
    "    embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "historical_rmse, historical_r2, historical_loss = np.inf, np.inf, np.inf\n",
    "for epoch in range(1, 20):\n",
    "    loss = train()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        temp_embedding = model.full_forward(x.to(device), edge_index.to(device)).cpu()\n",
    "        regression_rmse, regression_r2 = regression_train(temp_embedding)\n",
    "\n",
    "        if historical_rmse == np.inf:\n",
    "            historical_rmse = regression_rmse\n",
    "\n",
    "        if historical_r2 == np.inf:\n",
    "            historical_r2 = regression_r2\n",
    "\n",
    "        if historical_loss == np.inf:\n",
    "            historical_loss = loss\n",
    "\n",
    "        if loss > historical_loss:\n",
    "            # if regression_rmse > historical_rmse:\n",
    "            #     print(\n",
    "            #         f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on RMSE! Current RMSE is {regression_rmse}, previous RMSE is {historical_rmse} ')\n",
    "            #     break\n",
    "\n",
    "            if regression_r2 < historical_r2:\n",
    "                print(\n",
    "                    f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Stopped on r2! Current R2 is {regression_r2}, previous R2 is {historical_r2} ')\n",
    "                break\n",
    "        else:\n",
    "            historical_loss = loss\n",
    "            historical_rmse = regression_rmse\n",
    "            historical_r2 = regression_r2\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f},Current RMSE is {regression_rmse}, Current R2 is {regression_r2} ')\n",
    "        output_embedding = temp_embedding\n",
    "\n",
    "# output_embedding = get_model_embedding()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(output_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_np = output_embedding.numpy()  #convert to Numpy array\n",
    "output_df = pd.DataFrame(output_np)  #convert to a dataframe\n",
    "current_GMT = time.gmtime()\n",
    "ts = calendar.timegm(current_GMT)\n",
    "output_df.to_csv(f\"./outputs/akl_embedding_{ts}.csv\", index=False)  #save to file\n",
    "print(f\"akl_embedding_{ts}.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
