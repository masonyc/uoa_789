{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler as RawNeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import osmnx as ox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   restaurant  amenity  school  shop  healthcare  clothes\n",
      "0         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "1         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "2         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "3         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "4         0.0      0.0     0.0   0.0         0.0      0.0\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "458252\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "street_nodes_df = pd.read_csv(\"./outputs/akl_street_nodes.csv\")\n",
    "street_nodes_df = street_nodes_df[street_nodes_df.columns[4:]]\n",
    "\n",
    "street_nodes_df_copy = street_nodes_df.copy()\n",
    "street_nodes_df_copy.drop([\"street_length\", \"Average_POI_Distance\", \"x\", \"y\"], axis=1, inplace=True)\n",
    "print(street_nodes_df_copy.head())\n",
    "\n",
    "street_nodes_features_tensor = torch.tensor(street_nodes_df_copy.values.tolist())\n",
    "number_of_nodes = len(street_nodes_features_tensor)\n",
    "number_of_node_features = len(street_nodes_features_tensor[0])\n",
    "print(street_nodes_features_tensor)\n",
    "print(number_of_nodes)\n",
    "print(number_of_node_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,      2,      0,  ..., 458250, 458250, 458251],\n",
      "        [     2,      0,      1,  ..., 458249, 458251, 458250]])\n",
      "tensor([ 96.8730,  96.8730, 100.7870,  ..., 328.7770, 493.5280, 493.5280])\n"
     ]
    }
   ],
   "source": [
    "street_edges_df = pd.read_csv(\"./outputs/akl_street_edges.csv\")\n",
    "source_street_index, targe_street_index, street_distance_weight = street_edges_df[\"source_street\"], street_edges_df[\n",
    "    \"target_street\"], street_edges_df[\"distance\"]\n",
    "street_edges_source_index_tensor = torch.tensor([source_street_index.values.tolist()])\n",
    "street_edges_target_index_tensor = torch.tensor([targe_street_index.values.tolist()])\n",
    "street_edges_index_tensor = torch.cat((street_edges_source_index_tensor, street_edges_target_index_tensor), 0)\n",
    "street_edges_weight_tensor = torch.tensor(street_distance_weight.values.tolist())\n",
    "print(street_edges_index_tensor)\n",
    "print(street_edges_weight_tensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "outputs": [],
   "source": [
    "def custom_pos_sampling(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    ") -> Union[Tensor, Tuple[Tensor, Tensor]]:\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        total_distance = 0\n",
    "        current_node_id = start_node_id\n",
    "        # 在edge文件里 对应的id 要 -1 比如neighbour是0， 在文件里index是1\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        # 选出edge对应的weight\n",
    "        neighbour_weights = torch.index_select(edge_weight, 0, neighbours_edge_index)\n",
    "        norm_neighbour_weights = [i / sum(neighbour_weights.numpy()) for i in neighbour_weights.numpy()]\n",
    "        #根据概率随机选一个\n",
    "        #print(neighbours_edge_index,len(neighbour_weights))\n",
    "        if len(neighbour_weights) == 0:\n",
    "            current_node_seq.append(current_node_id)\n",
    "            pos_node_seq.append(current_node_seq)\n",
    "            #neg_node_seq.append(current_node_seq)\n",
    "            continue\n",
    "        neighbour_weights_index = np.random.choice(len(neighbour_weights), p=norm_neighbour_weights)\n",
    "\n",
    "        # print(\"current Node id \\n\", current_node_id)\n",
    "        #print(\"neighbour weights \\n\", neighbour_weights)\n",
    "        #print(\"neighbour weights index  \\n\", neighbour_weights_index)\n",
    "        #print(neighbour_weights.min(),neighbour_weights.argmin())\n",
    "\n",
    "        # 取最近的边\n",
    "        # TODO：加入别的策略，poi信息等\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "        #print(\"next edge \\n\", next_edge_df)\n",
    "        next_edge = next_edge_df.values[0]\n",
    "        total_distance += next_edge[2]\n",
    "        # next_edge[0] = source street\n",
    "        # next_edge[1] = target_street\n",
    "        # next_edge[2] = distance\n",
    "        if next_edge[0] != current_node_id:\n",
    "            current_node_id = next_edge[0]\n",
    "        else:\n",
    "            current_node_id = next_edge[1]\n",
    "        current_node_seq.append(current_node_id)\n",
    "        pos_node_seq.append(current_node_seq)\n",
    "    #if len(neg_node_seq) >0 :\n",
    "    #print(\"Isolated node: {number} {node_list}\".format(number = len(neg_node_seq),node_list = neg_node_seq))\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "def custom_sampling_with_POI(\n",
    "        edge_weight: Tensor,\n",
    "        batch: Tensor,\n",
    ") :\n",
    "    pos_node_seq = []\n",
    "    neg_node_seq = []\n",
    "    poi_nodes = set()\n",
    "    no_poi_nodes=set()\n",
    "    for start_node_id in batch:\n",
    "        current_node_seq = [start_node_id.item()]\n",
    "        current_node_id = current_node_seq[-1]\n",
    "        # 找距离？\n",
    "        # current_x,current_y = street_nodes_df.iloc[[start_node_id]][\"x\"],street_nodes_df.iloc[[start_node_id]][\"y\"]\n",
    "        # print(start_node_id)\n",
    "        # print(street_nodes_df.iloc[[start_node_id]])\n",
    "        neighbours_edge_index = (street_edges_index_tensor == current_node_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "        neighbour_id_list = []\n",
    "        neighbour_id_index = []\n",
    "        for edge_index in neighbours_edge_index:\n",
    "            neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "            neighbour_edge = neighbour_edge_df.values[0]\n",
    "            if neighbour_edge[0] != current_node_id:\n",
    "                neighbour_id = neighbour_edge[0]\n",
    "            else:\n",
    "                neighbour_id = neighbour_edge[1]\n",
    "            # neighbour_id_list.append([neighbour_id])\n",
    "            neighbour_id_list.append(neighbour_id)\n",
    "            neighbour_id_index.append(0)\n",
    "\n",
    "        # steps 自动-1 比如想要3步的话 就传2\n",
    "        # neighbour_id_list = find_neighbours(3, neighbour_id_list, neighbour_id_index)#,current_x,current_y,500)\n",
    "        neighbour_id_weights = []\n",
    "        # for neighbour_ids in neighbour_id_list:\n",
    "        poi_weight = 0\n",
    "        for neighbour_id in neighbour_id_list:\n",
    "            neighbour_features = torch.index_select(street_nodes_features_tensor, 0,\n",
    "                                                    torch.tensor(int(neighbour_id), dtype=torch.int32))\n",
    "            poi_weight += torch.sum(neighbour_features)\n",
    "        neighbour_id_weights.append(poi_weight)\n",
    "\n",
    "        # print(f\"neighbour_id_weights = {neighbour_id_weights}\")\n",
    "        neighbour_id_weights = np.array(neighbour_id_weights)\n",
    "        normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n",
    "\n",
    "        neighbour_weights_index = 0\n",
    "\n",
    "        if np.isnan(normalized_neighbour_weights).all():\n",
    "            no_poi_nodes.add(current_node_id)\n",
    "            if len(neighbours_edge_index) == 0:\n",
    "                current_node_seq.append(current_node_id)\n",
    "                pos_node_seq.append(current_node_seq)\n",
    "                if len(poi_nodes) == 0:\n",
    "                    init_neg_node = street_nodes_df.sample()\n",
    "                    # print(init_neg_node.index.values[0])\n",
    "                    neg_node_seq.append(init_neg_node.index.values[0])\n",
    "                else:\n",
    "                    neg_node_seq.append(np.random.choice(list(poi_nodes)))\n",
    "                continue\n",
    "            else:\n",
    "                neighbour_weights_index = np.random.choice(len(neighbours_edge_index))\n",
    "        else:\n",
    "            poi_nodes.add(current_node_id)\n",
    "            neighbour_weights_index = np.random.choice(len(normalized_neighbour_weights),\n",
    "                                                       p=normalized_neighbour_weights)\n",
    "\n",
    "        next_edge_index = neighbours_edge_index[neighbour_weights_index]\n",
    "        next_edge_df = street_edges_df.iloc[[next_edge_index]]\n",
    "        next_edge = next_edge_df.values[0]\n",
    "        if next_edge[0] != current_node_id:\n",
    "            current_node_id = next_edge[0]\n",
    "        else:\n",
    "            current_node_id = next_edge[1]\n",
    "        current_node_seq.append(current_node_id)\n",
    "        pos_node_seq.append(current_node_seq)\n",
    "        if len(no_poi_nodes) == 0:\n",
    "            init_neg_node = street_nodes_df.sample()\n",
    "            # print(init_neg_node.index.values[0])\n",
    "            neg_node_seq.append(init_neg_node.index.values[0])\n",
    "        else:\n",
    "            neg_node_seq.append(np.random.choice(list(no_poi_nodes)))\n",
    "    return torch.from_numpy(np.asarray(pos_node_seq, dtype=np.int32))[:, 1],torch.from_numpy(np.asarray(neg_node_seq,dtype=np.int32))\n",
    "\n",
    "\n",
    "# bfs like\n",
    "def find_neighbours(steps, neighbour_id_list, neighbour_id_index_list):#,origin_x,origin_y,max_dist):\n",
    "    if steps <= 0:\n",
    "        return neighbour_id_list\n",
    "\n",
    "    current = neighbour_id_list\n",
    "    neighbour_id_index = []\n",
    "    for i, neighbour_list in enumerate(current):\n",
    "        short_neighbour_list = neighbour_list[neighbour_id_index_list[i]:]\n",
    "        neighbour_id_index.append(len(neighbour_list) + 1)\n",
    "        for neigh in short_neighbour_list:\n",
    "            neighbours_edge_index = (street_edges_index_tensor == neigh).nonzero(as_tuple=True)[1]\n",
    "\n",
    "            for edge_index in neighbours_edge_index:\n",
    "                neighbour_edge_df = street_edges_df.iloc[[edge_index]]\n",
    "                neighbour_edge = neighbour_edge_df.values[0]\n",
    "                if neighbour_edge[0] != neigh:\n",
    "                    neighbour_id = neighbour_edge[0]\n",
    "                else:\n",
    "                    neighbour_id = neighbour_edge[1]\n",
    "\n",
    "                # 找距离？\n",
    "                # loc_x,loc_y = street_nodes_df.iloc[[neighbour_id]][\"x\"],street_nodes_df.iloc[[neighbour_id]][\"y\"]\n",
    "                # distance_to_origin = ox.distance.euclidean_dist_vec(loc_y,loc_x,\n",
    "                #                                                  origin_y,origin_x)\n",
    "                # if distance_to_origin > max_dist:\n",
    "                #     continue\n",
    "\n",
    "                neighbour_id_list[i].append(neighbour_id)\n",
    "    return find_neighbours(steps - 1, neighbour_id_list, neighbour_id_index) #,origin_x,origin_y,max_dist)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RawNeighborSampler This module iteratively samples neighbors (at each layer) and constructs bipartite graphs that simulate the actual computation flow of GNNs.\n",
    "\n",
    "sizes: denotes how much neighbors we want to sample for each node in each layer.\n",
    "\n",
    "NeighborSampler holds the current :obj:batch_size, the IDs :obj:n_id of all nodes involved in the computation, and a list of bipartite graph objects via the tuple :obj:(edge_index, e_id, size), where :obj:edge_index represents the bipartite edges between source and target nodes, :obj:e_id denotes the IDs of original edges in the full graph, and :obj:size holds the shape of the bipartite graph.\n",
    "\n",
    "The actual computation graphs are then returned in reverse-mode, meaning that we pass messages from a larger set of nodes to a smaller one, until we reach the nodes for which we originally wanted to compute embeddings.\n",
    "https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class NeighborSampler(RawNeighborSampler):\n",
    "    def sample(self, batch):\n",
    "        batch = torch.tensor(batch)\n",
    "        row, col, _ = self.adj_t.coo()\n",
    "\n",
    "        pos_batch,neg_batch = custom_sampling_with_POI(street_edges_weight_tensor, batch)\n",
    "        #neg_batch = custom_neg_sampling(street_edges_weight_tensor, batch, self.adj_t.size(1))\n",
    "        # neg_batch = torch.randint(0, self.adj_t.size(1), (batch.numel(),), dtype=torch.long)\n",
    "        #print(\"Custom nodes seq,\", pos_batch)\n",
    "        #print(\"negative batch \\n \", neg_batch)\n",
    "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
    "        sampled = super().sample(batch)\n",
    "        return sampled\n",
    "\n",
    "\n",
    "train_loader = NeighborSampler(street_edges_index_tensor, sizes=[8, 8], batch_size=256,\n",
    "                               shuffle=True, num_nodes=number_of_nodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SAGE(number_of_node_features, hidden_channels=128, num_layers=5)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x, edge_index = street_nodes_features_tensor.to(device), street_edges_index_tensor.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/__j58r2568d6gfl4x7l64qwh0000gn/T/ipykernel_17877/4211014146.py:44: RuntimeWarning: invalid value encountered in true_divide\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n",
      "/var/folders/qh/__j58r2568d6gfl4x7l64qwh0000gn/T/ipykernel_17877/4211014146.py:44: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  normalized_neighbour_weights = [i / sum(neighbour_id_weights) for i in neighbour_id_weights]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.3919, \n",
      "Epoch: 002, Loss: 1.3872, \n",
      "Epoch: 003, Loss: 1.3870, \n",
      "Epoch: 004, Loss: 1.3916, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [315]\u001B[0m, in \u001B[0;36m<cell line: 34>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m embedding\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m15\u001B[39m):\n\u001B[0;32m---> 35\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m03d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     38\u001B[0m output_embedding \u001B[38;5;241m=\u001B[39m get_model_embedding()\n",
      "Input \u001B[0;32mIn [315]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      5\u001B[0m i \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_size, n_id, adjs \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m      7\u001B[0m     i \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m#print(i)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# `adjs` holds a list of `(edge_index, e_id, size)` tuples.\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [313]\u001B[0m, in \u001B[0;36mNeighborSampler.sample\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     15\u001B[0m batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(batch)\n\u001B[1;32m     16\u001B[0m row, col, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madj_t\u001B[38;5;241m.\u001B[39mcoo()\n\u001B[0;32m---> 18\u001B[0m pos_batch,neg_batch \u001B[38;5;241m=\u001B[39m \u001B[43mcustom_sampling_with_POI\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstreet_edges_weight_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m#neg_batch = custom_neg_sampling(street_edges_weight_tensor, batch, self.adj_t.size(1))\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# neg_batch = torch.randint(0, self.adj_t.size(1), (batch.numel(),), dtype=torch.long)\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m#print(\"Custom nodes seq,\", pos_batch)\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m#print(\"negative batch \\n \", neg_batch)\u001B[39;00m\n\u001B[1;32m     23\u001B[0m batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([batch, pos_batch, neg_batch], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "Input \u001B[0;32mIn [312]\u001B[0m, in \u001B[0;36mcustom_sampling_with_POI\u001B[0;34m(edge_weight, batch)\u001B[0m\n\u001B[1;32m     19\u001B[0m neighbour_id_index \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m edge_index \u001B[38;5;129;01min\u001B[39;00m neighbours_edge_index:\n\u001B[0;32m---> 21\u001B[0m     neighbour_edge_df \u001B[38;5;241m=\u001B[39m \u001B[43mstreet_edges_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     22\u001B[0m     neighbour_edge \u001B[38;5;241m=\u001B[39m neighbour_edge_df\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m neighbour_edge[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m current_node_id:\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/pandas/core/indexing.py:967\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    964\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    966\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[0;32m--> 967\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/pandas/core/indexing.py:1511\u001B[0m, in \u001B[0;36m_iLocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1509\u001B[0m \u001B[38;5;66;03m# a list of integers\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_list_like_indexer(key):\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_list_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1513\u001B[0m \u001B[38;5;66;03m# a single integer\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1515\u001B[0m     key \u001B[38;5;241m=\u001B[39m item_from_zerodim(key)\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/pandas/core/indexing.py:1482\u001B[0m, in \u001B[0;36m_iLocIndexer._get_list_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1465\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1466\u001B[0m \u001B[38;5;124;03mReturn Series values by list or array of integers.\u001B[39;00m\n\u001B[1;32m   1467\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1479\u001B[0m \u001B[38;5;124;03m`axis` can only be zero.\u001B[39;00m\n\u001B[1;32m   1480\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1481\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take_with_is_copy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m   1484\u001B[0m     \u001B[38;5;66;03m# re-raise with different error message\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpositional indexers are out-of-bounds\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/pandas/core/generic.py:3716\u001B[0m, in \u001B[0;36mNDFrame._take_with_is_copy\u001B[0;34m(self, indices, axis)\u001B[0m\n\u001B[1;32m   3708\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_take_with_is_copy\u001B[39m(\u001B[38;5;28mself\u001B[39m: NDFrameT, indices, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NDFrameT:\n\u001B[1;32m   3709\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3710\u001B[0m \u001B[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001B[39;00m\n\u001B[1;32m   3711\u001B[0m \u001B[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3714\u001B[0m \u001B[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001B[39;00m\n\u001B[1;32m   3715\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3716\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3717\u001B[0m     \u001B[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001B[39;00m\n\u001B[1;32m   3718\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result\u001B[38;5;241m.\u001B[39m_get_axis(axis)\u001B[38;5;241m.\u001B[39mequals(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis(axis)):\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/pandas/core/generic.py:3703\u001B[0m, in \u001B[0;36mNDFrame.take\u001B[0;34m(self, indices, axis, is_copy, **kwargs)\u001B[0m\n\u001B[1;32m   3699\u001B[0m nv\u001B[38;5;241m.\u001B[39mvalidate_take((), kwargs)\n\u001B[1;32m   3701\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consolidate_inplace()\n\u001B[0;32m-> 3703\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3704\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_block_manager_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m   3705\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3706\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor(new_data)\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtake\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/pandas/core/internals/managers.py:900\u001B[0m, in \u001B[0;36mBaseBlockManager.take\u001B[0;34m(self, indexer, axis, verify)\u001B[0m\n\u001B[1;32m    897\u001B[0m indexer \u001B[38;5;241m=\u001B[39m maybe_convert_indices(indexer, n, verify\u001B[38;5;241m=\u001B[39mverify)\n\u001B[1;32m    899\u001B[0m new_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes[axis]\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m--> 900\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreindex_indexer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    901\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnew_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindexer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    903\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    904\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_dups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    905\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    906\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/research789/lib/python3.9/site-packages/pandas/core/internals/managers.py:634\u001B[0m, in \u001B[0;36mBaseBlockManager.reindex_indexer\u001B[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)\u001B[0m\n\u001B[1;32m    631\u001B[0m     bm\u001B[38;5;241m.\u001B[39m_consolidate_inplace()\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bm\n\u001B[0;32m--> 634\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreindex_indexer\u001B[39m(\n\u001B[1;32m    635\u001B[0m     \u001B[38;5;28mself\u001B[39m: T,\n\u001B[1;32m    636\u001B[0m     new_axis: Index,\n\u001B[1;32m    637\u001B[0m     indexer,\n\u001B[1;32m    638\u001B[0m     axis: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m    639\u001B[0m     fill_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    640\u001B[0m     allow_dups: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    641\u001B[0m     copy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    642\u001B[0m     consolidate: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    643\u001B[0m     only_slice: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    644\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m    645\u001B[0m     use_na_proxy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    646\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    647\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    648\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m    649\u001B[0m \u001B[38;5;124;03m    ----------\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;124;03m    pandas-indexer with -1's only.\u001B[39;00m\n\u001B[1;32m    664\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    665\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m indexer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        i += 1\n",
    "        #print(i)\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(x[n_id], adjs)\n",
    "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "    # print(i)\n",
    "    return total_loss / number_of_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_model_embedding():\n",
    "    model.eval()\n",
    "    embedding = model.full_forward(x, edge_index).cpu()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "for epoch in range(1, 15):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, ')\n",
    "\n",
    "output_embedding = get_model_embedding()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(output_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_np = output_embedding.numpy()  #convert to Numpy array\n",
    "output_df = pd.DataFrame(output_np)  #convert to a dataframe\n",
    "output_df.to_csv(\"./outputs/akl_embedding.csv\", index=False)  #save to file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
